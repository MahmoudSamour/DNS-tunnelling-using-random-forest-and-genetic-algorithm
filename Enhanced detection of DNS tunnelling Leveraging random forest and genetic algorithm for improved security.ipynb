{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ve2hwdLRokfe"
   },
   "source": [
    "# Enhanced detection of DNS tunnelling: Leveraging random forest and genetic algorithm for improved security"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xr9XZvver6gp"
   },
   "source": [
    "To download the necessary CSV files for the project, use the following `wget` commands:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLL6gQY85FFF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# List of file URLs to download\n",
    "file_urls = [\n",
    "    (\"https://drive.usercontent.google.com/download?id=1cictwnxUyu1vCa4H9iefIrQeVLCC3RCv&export=download&authuser=0&confirm=t&uuid=8ec5d698-4d5d-4592-94eb-8a82234966ac&at=AC2mKKTzwehwnBUepaEJIDoKDql-:1690876827674\", \"benign-chrome.csv\"),\n",
    "    (\"https://drive.usercontent.google.com/download?id=1cms99qEylyvesqcX3dQRZOUQRAONy2uS&export=download&authuser=0&confirm=t&uuid=0f089685-41f1-40fe-903e-8fcc8e2bcac8&at=AC2mKKSfqH9g0sjW4mQVa5-J4gMf:1690877149684\", \"benign-firefox.csv\"),\n",
    "    (\"https://drive.usercontent.google.com/download?id=1cqDL7A_kdOCL4Km4uUifRPllFmB3WaZ_&export=download&authuser=0&confirm=t&uuid=19171c97-ad00-4af4-bf46-ef8c453b2964&at=AC2mKKROICucTfu1coxAIff16wi1:1690878058234\", \"mal-dns2tcp.csv\"),\n",
    "    (\"https://drive.usercontent.google.com/download?id=1cxeTvXNV-OY_4T6xs4sUB98lmanROw3m&export=download&authuser=0&confirm=t&uuid=67df7c64-15ed-450d-bad8-f416080d378d&at=AC2mKKST9kQGoFcvwe9EhJoY6jRA:1690878087508\", \"mal-dnscat2.csv\"),\n",
    "    (\"https://drive.google.com/u/1/uc?id=1czNRMpNyicFNYW2fbK_WjsoF77qB9_XA&export=download\", \"mal-iodine.csv\")\n",
    "]\n",
    "\n",
    "# Create a directory to save the files\n",
    "if not os.path.exists(\"DoHBrw-2020\"):\n",
    "    os.makedirs(\"DoHBrw-2020\")\n",
    "\n",
    "# Loop through the file URLs and download files if not already present\n",
    "for url, filename in file_urls:\n",
    "    file_path = os.path.join(\"DoHBrw-2020\", filename)\n",
    "    if not os.path.exists(file_path):\n",
    "        try:\n",
    "            print(f\"Downloading {filename}...\")\n",
    "            response = requests.get(url, stream=True)\n",
    "            with open(file_path, \"wb\") as file:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    file.write(chunk)\n",
    "            print(f\"{filename} downloaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {filename}: {e}\")\n",
    "    else:\n",
    "        print(f\"{filename} already exists!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vs6ZiJsAosz4"
   },
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import json  # For working with JSON data\n",
    "import math  # For mathematical operations\n",
    "from collections import Counter  # For counting elements in a list\n",
    "from os.path import join  # For joining file paths\n",
    "import numpy as np  # For numerical operations and arrays\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import plotly.express as px  # For interactive plotting\n",
    "import plotly.figure_factory as ff  # For creating various types of figures\n",
    "import plotly.graph_objects as go  # For creating customized plots\n",
    "import random  # For generating random values\n",
    "from tqdm.notebook import tqdm, trange  # For displaying progress bars in Jupyter Notebook\n",
    "from deap import base, creator, tools, algorithms  # For evolutionary algorithms\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, f1_score  # For model evaluation metrics\n",
    "from sklearn.model_selection import cross_val_score, train_test_split  # For cross-validation and data splitting\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder  # For data preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier  # For building a Random Forest classifier\n",
    "from sklearn.impute import SimpleImputer  # For imputing missing values\n",
    "from sklearn.inspection import permutation_importance  # For feature importance analysis\n",
    "\n",
    "from plotly.offline import iplot  # For offline plotting\n",
    "\n",
    "# Additional imports\n",
    "import matplotlib.pyplot as plt  # For creating traditional plots\n",
    "\n",
    "# Shuffle data\n",
    "from sklearn.utils import shuffle  # For shuffling data\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import pickle  # Add this import statement\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "noSrJ0VCouHL"
   },
   "outputs": [],
   "source": [
    "# Read the first benign CSV file into a DataFrame called df1_benign\n",
    "df1_benign = pd.read_csv('DoHBrw-2020/benign-chrome.csv', delimiter=',')\n",
    "\n",
    "# Read the second benign CSV file into another DataFrame called df2_benign\n",
    "df2_benign = pd.read_csv('DoHBrw-2020/benign-firefox.csv', delimiter=',')\n",
    "\n",
    "# Append the contents of df2_benign to df1_benign (Note: This does not modify df1_benign in-place, it returns a new DataFrame)\n",
    "df_benign = pd.concat([df1_benign, df2_benign], ignore_index=True)\n",
    "\n",
    "# Add a new column 'DoH' to df1_benign and set all values in that column to 0, indicating benign traffic\n",
    "df_benign['DoH'] = 0  # 'DoH' stands for DNS-over-HTTPS, and 0 indicates benign traffic\n",
    "\n",
    "# Rename the column 'DoH' to 'labels' in df1_benign\n",
    "df_benign = df_benign.rename(columns={'DoH': 'labels'})\n",
    "df_benign\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTuUD1wKovWS"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read the first malicious CSV file into a DataFrame called df1_malic\n",
    "df1_malic = pd.read_csv('DoHBrw-2020/mal-iodine.csv', delimiter=',')\n",
    "\n",
    "# Add a new column 'DoH' to df1_malic and set all values in that column to 1, indicating malicious traffic of type 'iodine'\n",
    "df1_malic['DoH'] = 1  # 1 stands for 'iodine' (a type of malicious traffic)\n",
    "\n",
    "# Read the second malicious CSV file into another DataFrame called df2_malic\n",
    "df2_malic = pd.read_csv('DoHBrw-2020/mal-dns2tcp.csv', delimiter=',')\n",
    "\n",
    "# Add a new column 'DoH' to df2_malic and set all values in that column to 2, indicating malicious traffic of type 'dns2tcp'\n",
    "df2_malic['DoH'] = 2  # 2 stands for 'dns2tcp' (another type of malicious traffic)\n",
    "\n",
    "# Read the third malicious CSV file into another DataFrame called df3_malic\n",
    "df3_malic = pd.read_csv('DoHBrw-2020/mal-dnscat2.csv', delimiter=',')\n",
    "\n",
    "# Add a new column 'DoH' to df3_malic and set all values in that column to 3, indicating malicious traffic of type 'dnscat2'\n",
    "df3_malic['DoH'] = 3  # 3 stands for 'dnscat2' (yet another type of malicious traffic)\n",
    "\n",
    "# Concatenate the DataFrames df1_malic, df2_malic, and df3_malic into a single DataFrame\n",
    "# The 'ignore_index=True' ensures that the index is reset after concatenation to avoid index duplication\n",
    "df1_malic = pd.concat([df1_malic, df2_malic, df3_malic], ignore_index=True)\n",
    "\n",
    "# Rename the column 'DoH' to 'labels' in df1_malic to have a common label indicating the type of traffic (0 for benign, 1, 2, 3 for malicious types)\n",
    "df1_malic = df1_malic.rename(columns={'DoH': 'labels'})\n",
    "df1_malic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GROAVHRowte"
   },
   "outputs": [],
   "source": [
    "# Shuffle the DataFrame\n",
    "data = shuffle(pd.concat([df_benign, df1_malic], ignore_index=True))\n",
    "\n",
    "# Check the number of null (missing) values in each column of the DataFrame 'data'\n",
    "null_value_counts = data.isnull().sum()\n",
    "\n",
    "# Drop columns with the same value across all rows\n",
    "columns_to_drop = [col for col in data.columns if data[col].nunique() == 1]\n",
    "data_dropped = data.drop(columns=columns_to_drop)\n",
    "\n",
    "# Fill missing values or NaN values with 0 for all columns\n",
    "data_filled = data_dropped.fillna(0)\n",
    "\n",
    "# Print the number of null values after filling\n",
    "print(\"Null Value Counts after Filling:\")\n",
    "print(data_filled.isnull().sum())\n",
    "\n",
    "# Now 'data_filled' contains the DataFrame with missing values filled with 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Icdw-9Xjw8fL"
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tHprQBWCoy15"
   },
   "outputs": [],
   "source": [
    "# Compute the statistical summary of numeric columns in the DataFrame 'data'\n",
    "data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-pFuASzo1YI"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The code data['SourceIP'] is used to access the 'SourceIP' column in the DataFrame data.\n",
    "It retrieves the values of the 'SourceIP' column, which represents the source IP addresses of\n",
    "the network traffic data.\n",
    "\"\"\"\n",
    "data['SourceIP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4vt_x17o0E6"
   },
   "outputs": [],
   "source": [
    "# Compute the count of each unique value in the 'labels' column of the DataFrame 'data'\n",
    "data.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0cG_Ym61HEo-"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Map the numeric labels to their corresponding descriptions\n",
    "attack_descriptions = {\n",
    "    0: \"Benign\",\n",
    "    1: \"Malicious - Iodine\",\n",
    "    2: \"Malicious - DNS2TCP\",\n",
    "    3: \"Malicious - Dnscat2\",\n",
    "}\n",
    "\n",
    "# Convert the 'TimeStamp' column to datetime if it's not already in datetime format\n",
    "data['TimeStamp'] = pd.to_datetime(data['TimeStamp'])\n",
    "\n",
    "# Group the data by 'TimeStamp' and 'labels' to get the count of each attack type at each timestamp\n",
    "grouped_data = data.groupby(['TimeStamp', 'labels']).size().reset_index(name='count')\n",
    "\n",
    "# Create a new column 'AttackTypeDescription' by mapping the 'labels' to their corresponding descriptions\n",
    "grouped_data['AttackTypeDescription'] = grouped_data['labels'].map(attack_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542,
     "output_embedded_package_id": "1ZJTPdRiDRkZTbQYLFddHSj3M0WCSiFoQ"
    },
    "executionInfo": {
     "elapsed": 9246,
     "status": "ok",
     "timestamp": 1692083022512,
     "user": {
      "displayName": "Mahmoud Sammour",
      "userId": "07606788054442128619"
     },
     "user_tz": -180
    },
    "id": "bGEiMNlAyrvB",
    "outputId": "16ed4046-e6c6-4bd1-e84c-71513db42f96"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the plot\n",
    "fig = px.line(\n",
    "    grouped_data,\n",
    "    x='TimeStamp',\n",
    "    y='count',\n",
    "    color='AttackTypeDescription',\n",
    "    markers=True,\n",
    "    hover_data={'AttackTypeDescription': True},  # Show attack descriptions on hover\n",
    ")\n",
    "\n",
    "# Update the layout for better readability (optional)\n",
    "fig.update_layout(\n",
    "    title='Attack Type Distribution Over Time',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='Count',\n",
    "    legend_title='Attack Type',\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxHxTvzio5gY"
   },
   "outputs": [],
   "source": [
    "# Create an instance of LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Iterate over all columns in the DataFrame\n",
    "for column in data.columns:\n",
    "    # Check if the column is non-numeric (categorical)\n",
    "    if data[column].dtype == 'object':\n",
    "        # Fit and transform the column using LabelEncoder\n",
    "        data[column] = le.fit_transform(data[column])\n",
    "\n",
    "# Now, the non-numeric columns have been converted to numerical labels\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O391HSvwo8dM"
   },
   "outputs": [],
   "source": [
    "data['SourceIP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDA-Zc3Xo9oD"
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhFUvRLia_uq"
   },
   "outputs": [],
   "source": [
    "# Separate the data into different classes\n",
    "benign_data = data[data['labels'] == 0].head(100)\n",
    "malicious_data = data[data['labels'] != 0].head(300)\n",
    "\n",
    "# Combine the data samples\n",
    "small_sample = pd.concat([benign_data, malicious_data], ignore_index=True)\n",
    "\n",
    "# Print the small sample\n",
    "# data = small_sample.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uYU-On4Uo_j-"
   },
   "outputs": [],
   "source": [
    "# Create the feature variables (X) by dropping the \"TimeStamp\" and \"labels\" columns from the DataFrame 'data'\n",
    "#X = data.drop([\"TimeStamp\", \"labels\"], axis=1)\n",
    "X = data.drop([\"TimeStamp\", \"labels\"], axis=1)\n",
    "# 'data.drop([\"TimeStamp\", \"labels\"], axis=1)' removes the \"TimeStamp\" and \"labels\" columns from 'data' and returns a new DataFrame 'X'\n",
    "# The 'axis=1' parameter specifies that we want to drop columns, not rows.\n",
    "\n",
    "# Create the target variable (y) by extracting the values from the \"labels\" column of the DataFrame 'data'\n",
    "#y = data['labels'].values\n",
    "y = data['labels'].values\n",
    "\n",
    "# 'data['labels']' accesses the \"labels\" column in 'data', and '.values' extracts the values as a NumPy array.\n",
    "# The resulting 'y' will be a one-dimensional NumPy array containing the target labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KOHlhfLnpBL_"
   },
   "outputs": [],
   "source": [
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lvq34CbSpCg5"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets using a test size of 50% (0.5) of the entire dataset\n",
    "# The random_state parameter ensures reproducibility by fixing the random seed used for the split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.5, random_state=1)\n",
    "\n",
    "# Further split the training set into training and validation sets using a test size of 25% (0.25) of the training set\n",
    "# The random_state parameter ensures consistency between different runs by using the same random seed as before.\n",
    "# The validation set size will be 25% of 50% (0.25 x 0.5 = 0.125) of the entire dataset.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_Y3wpaNpEGG"
   },
   "outputs": [],
   "source": [
    "\n",
    "def displayClasificationResults(z, y_test, y_pred, numClasses=4):\n",
    "    # Calculate and display the number of mislabeled points and accuracy\n",
    "    print(\"Number of mislabeled points out of a total %d points: %d\"\n",
    "          % (y_test.shape[0], (y_test != y_pred).sum()))\n",
    "    accuracy = round(100 - (((y_test != y_pred).sum() / y_test.shape[0]) * 100), 2)\n",
    "    print(f\"Accuracy is {accuracy}%\")\n",
    "\n",
    "    # Calculate and display precision, recall, and F-score (weighted average)\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average='weighted')\n",
    "    precision *= 100\n",
    "    recall *= 100\n",
    "    fscore *= 100\n",
    "    print(f\"Precision = {round(precision, 2)}%\")\n",
    "    print(f\"Recall = {round(recall, 2)}%\")\n",
    "    print(f\"F-score = {round(fscore, 2)}%\")\n",
    "\n",
    "    # Set the labels for x and y axes in the confusion matrix\n",
    "    if numClasses == 2:\n",
    "        x = ['benign', 'malicious']\n",
    "        y = ['benign', 'malicious']\n",
    "    else:\n",
    "        x = ['benign', 'iodine', 'dns2tcp', 'dnscat2']\n",
    "        y = ['benign', 'iodine', 'dns2tcp', 'dnscat2']\n",
    "\n",
    "    # Change each element of z to type string for annotations in the heatmap\n",
    "    z_text = [[str(y) for y in x] for x in z]\n",
    "\n",
    "    # Create an annotated heatmap using Plotly with the confusion matrix\n",
    "    fig = ff.create_annotated_heatmap(\n",
    "        z, x=x, y=y, annotation_text=z_text, colorscale='Viridis')\n",
    "\n",
    "    # Add title and custom axis titles to the heatmap\n",
    "    fig.update_layout(title_text='<i><b>Confusion matrix</b></i>')\n",
    "\n",
    "    # Add custom x-axis title\n",
    "    fig.add_annotation(dict(font=dict(color=\"black\", size=14),\n",
    "                            x=0.5,\n",
    "                            y=-0.15,\n",
    "                            showarrow=False,\n",
    "                            text=\"Predicted value\",\n",
    "                            xref=\"paper\",\n",
    "                            yref=\"paper\"))\n",
    "\n",
    "    # Add custom y-axis title with angle adjustment\n",
    "    fig.add_annotation(dict(font=dict(color=\"black\", size=14),\n",
    "                            x=-0.35,\n",
    "                            y=0.5,\n",
    "                            showarrow=False,\n",
    "                            text=\"Real value\",\n",
    "                            textangle=-90,\n",
    "                            xref=\"paper\",\n",
    "                            yref=\"paper\"))\n",
    "\n",
    "    # Adjust margins to make room for the y-axis title\n",
    "    fig.update_layout(margin=dict(t=50, l=200))\n",
    "\n",
    "    # Add colorbar to the heatmap\n",
    "    fig['data'][0]['showscale'] = True\n",
    "\n",
    "    # Show the heatmap\n",
    "    iplot(fig)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZRQvlGQJTeJ7"
   },
   "outputs": [],
   "source": [
    "# Create a RandomForestClassifier with 500 estimators and a fixed random state for reproducibility\n",
    "# rfc_4_classification = RandomForestClassifier(n_estimators=500, random_state=1)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc_4_classification = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    max_features='sqrt',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5C21p_LMWV_8"
   },
   "outputs": [],
   "source": [
    "# Train the RandomForestClassifier on the training data (X_train, y_train) and make predictions on the training data\n",
    "y_pred = rfc_4_classification.fit(X_train, y_train).predict(X_train)\n",
    "\n",
    "# Compute the confusion matrix using the actual training labels (y_train) and the predicted labels (y_pred)\n",
    "z = confusion_matrix(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yrKEWhCepQfl"
   },
   "outputs": [],
   "source": [
    "# Display the classification results using the 'displayClasificationResults' function\n",
    "# The function will show the number of mislabeled points, accuracy, precision, recall, and an annotated heatmap of the confusion matrix.\n",
    "displayClasificationResults(z, y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_Y0HNazpSGY"
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test data (X_test) using the trained RandomForestClassifier\n",
    "y_pred = rfc_4_classification.predict(X_test)\n",
    "\n",
    "# Compute the confusion matrix using the actual test labels (y_test) and the predicted labels (y_pred)\n",
    "z = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display the classification results using the 'displayClasificationResults' function\n",
    "# The function will show the number of mislabeled points, accuracy, precision, recall, and an annotated heatmap of the confusion matrix.\n",
    "displayClasificationResults(z, y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EhYEw5o8pTNu"
   },
   "outputs": [],
   "source": [
    "# Make predictions on the validation data (X_val) using the trained RandomForestClassifier\n",
    "y_pred = rfc_4_classification.predict(X_val)\n",
    "\n",
    "# Compute the confusion matrix using the actual validation labels (y_val) and the predicted labels (y_pred)\n",
    "z = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "# Display the classification results using the 'displayClasificationResults' function\n",
    "# The function will show the number of mislabeled points, accuracy, precision, recall, and an annotated heatmap of the confusion matrix.\n",
    "displayClasificationResults(z, y_val, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIBgVRRVpa_U"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate permutation feature importance\n",
    "perm_importance = permutation_importance(rfc_4_classification, X_test, y_test, n_repeats=30, random_state=1)\n",
    "\n",
    "# Obtain feature names\n",
    "feature_names = list(X.columns)\n",
    "\n",
    "# Sort features by importance scores\n",
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(sorted_idx)), perm_importance.importances_mean[sorted_idx], align='center')\n",
    "plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])\n",
    "plt.xlabel('Permutation Importance')\n",
    "plt.title('Feature Importance - Permutation Importance')\n",
    "plt.show()\n",
    "# Print feature names and their importance values in descending order\n",
    "for idx in reversed(sorted_idx):\n",
    "    print(f\"{feature_names[idx]}: {perm_importance.importances_mean[idx]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clv2TXVKyZQN"
   },
   "outputs": [],
   "source": [
    "# Print feature names and their importance values in descending order\n",
    "for idx in reversed(sorted_idx):\n",
    "    print(f\"{feature_names[idx]}: {perm_importance.importances_mean[idx]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJsXYYkK1Iah"
   },
   "outputs": [],
   "source": [
    "# Extract the top five feature names and their importance values\n",
    "top_feature_indices = sorted_idx[-5:]\n",
    "top_feature_names = [feature_names[idx] for idx in top_feature_indices]\n",
    "top_feature_importances = [perm_importance.importances_mean[idx] for idx in top_feature_indices]\n",
    "\n",
    "# Print the top five features and their importance values\n",
    "print(\"Top Five Features and Their Importance Values:\")\n",
    "for feature, importance in zip(top_feature_names, top_feature_importances):\n",
    "    print(f\"{feature}: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EKix7ZXt1LKo"
   },
   "outputs": [],
   "source": [
    "# Create a deep copy of data and name it newdata\n",
    "newdata = data.copy()\n",
    "\n",
    "# Create new combinations of the top five features using the mean\n",
    "new_feature_combinations = []\n",
    "for i in range(5):\n",
    "    for j in range(i + 1, 5):\n",
    "        new_combination = f\"{top_feature_names[i]}_{top_feature_names[j]}_mean\"\n",
    "        new_feature_combinations.append(new_combination)\n",
    "\n",
    "# Add the new feature combinations (mean) to the newdata dataframe\n",
    "for combination in new_feature_combinations:\n",
    "    feature_indices = [top_feature_names.index(name) for name in combination.split('_')[:-1]]\n",
    "    newdata[combination] = newdata[top_feature_names].iloc[:, feature_indices].mean(axis=1)\n",
    "\n",
    "\n",
    "# Print the first few rows of the dataframe to verify the additions\n",
    "print(\"Updated DataFrame with New Feature Combinations:\")\n",
    "newdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BBv73up5iuy"
   },
   "outputs": [],
   "source": [
    "# Prepare the new features and labels for machine learning\n",
    "X_new = newdata[new_feature_combinations].values\n",
    "y_new = newdata['labels'].values\n",
    "\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "X_train_all, X_temp_all, y_train_all, y_temp_all = train_test_split(X_new, y_new, test_size=0.5, random_state=1)\n",
    "X_val_all, X_test_all, y_val_all, y_test_all = train_test_split(X_temp_all, y_temp_all, test_size=0.25, random_state=1)\n",
    "\n",
    "# Create a new RandomForestClassifier for the updated dataset\n",
    "# rfc_new_all = RandomForestClassifier(n_estimators=500, random_state=1)\n",
    "rfc_new_all = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    max_features='sqrt',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "# Train the new classifier on the training data\n",
    "rfc_new_all.fit(X_train_all, y_train_all)\n",
    "\n",
    "# Make predictions on the training, validation, and testing data\n",
    "y_pred_train_all = rfc_new_all.predict(X_train_all)\n",
    "y_pred_val_all = rfc_new_all.predict(X_val_all)\n",
    "y_pred_test_all = rfc_new_all.predict(X_test_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBRMZ_TN1ZQV"
   },
   "outputs": [],
   "source": [
    "# Display classification results for the training data\n",
    "print(\"Classification Results for Training Data:\")\n",
    "displayClasificationResults(confusion_matrix(y_train_all, y_pred_train_all), y_train_all, y_pred_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZaTCASNs0iHr"
   },
   "outputs": [],
   "source": [
    "# Display classification results for the validation data\n",
    "print(\"Classification Results for Validation Data:\")\n",
    "displayClasificationResults(confusion_matrix(y_val_all, y_pred_val_all), y_val_all, y_pred_val_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YoOq4Th06iRx"
   },
   "outputs": [],
   "source": [
    "# Display classification results for the testing data\n",
    "print(\"Classification Results for Testing Data:\")\n",
    "displayClasificationResults(confusion_matrix(y_test_all, y_pred_test_all), y_test_all, y_pred_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4udLDOKD3Ks5"
   },
   "outputs": [],
   "source": [
    "# Create the feature variables (X) by dropping the \"TimeStamp\" and \"labels\" columns from the DataFrame 'data'\n",
    "X_all = newdata.drop([\"TimeStamp\", \"labels\"], axis=1)\n",
    "\n",
    "# Create the target variable (y) by extracting the values from the \"labels\" column of the DataFrame 'data'\n",
    "y_all = newdata['labels'].values\n",
    "X_imputed_all = imputer.fit_transform(X_all)\n",
    "X_scaled_all = scaler.fit_transform(X_imputed_all)\n",
    "\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "X_train_all, X_temp_all, y_train_all, y_temp_all = train_test_split(X_scaled_all, y_all, test_size=0.5, random_state=1)\n",
    "X_val_all, X_test_all, y_val_all, y_test_all = train_test_split(X_temp_all, y_temp_all, test_size=0.25, random_state=1)\n",
    "\n",
    "# Create a new RandomForestClassifier for the updated dataset\n",
    "# rfc_new_all = RandomForestClassifier(n_estimators=500, random_state=1)\n",
    "rfc_new_all = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    max_features='sqrt',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "# Train the new classifier on the training data\n",
    "rfc_new_all.fit(X_train_all, y_train_all)\n",
    "\n",
    "# Make predictions on the training, validation, and testing data\n",
    "y_pred_train_all = rfc_new_all.predict(X_train_all)\n",
    "y_pred_val_all = rfc_new_all.predict(X_val_all)\n",
    "y_pred_test_all = rfc_new_all.predict(X_test_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVCki0pZ8isk"
   },
   "outputs": [],
   "source": [
    "# Display classification results for the training data\n",
    "print(\"Classification Results for Training Data:\")\n",
    "displayClasificationResults(confusion_matrix(y_train_all, y_pred_train_all), y_train_all, y_pred_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ERXoiMb48lWZ"
   },
   "outputs": [],
   "source": [
    "# Display classification results for the validation data\n",
    "print(\"Classification Results for Validation Data:\")\n",
    "displayClasificationResults(confusion_matrix(y_val_all, y_pred_val_all), y_val_all, y_pred_val_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNJ56Xh78qoA"
   },
   "outputs": [],
   "source": [
    "# Display classification results for the testing data\n",
    "print(\"Classification Results for Testing Data:\")\n",
    "displayClasificationResults(confusion_matrix(y_test_all, y_pred_test_all), y_test_all, y_pred_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XijlrNi_-qtO"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calculate permutation feature importance\n",
    "perm_importance = permutation_importance(rfc_new_all, X_test_all, y_test_all, n_repeats=30, random_state=1)\n",
    "\n",
    "# Obtain feature names\n",
    "feature_names = list(X_all.columns)\n",
    "\n",
    "# Sort features by importance scores\n",
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(sorted_idx)), perm_importance.importances_mean[sorted_idx], align='center')\n",
    "plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])\n",
    "plt.xlabel('Permutation Importance')\n",
    "plt.title('Feature Importance - Permutation Importance')\n",
    "plt.show()\n",
    "# Print feature names and their importance values in descending order\n",
    "for idx in reversed(sorted_idx):\n",
    "    print(f\"{feature_names[idx]}: {perm_importance.importances_mean[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SNPPNz7vjQX5"
   },
   "outputs": [],
   "source": [
    "penalty_enabled=False\n",
    "\n",
    "# Function to evaluate an individual's fitness\n",
    "def evaluate(individual, X_train_selected, X_test_selected, y_train_all, y_test_all, penalty_enabled=True):\n",
    "    selected_features = [i for i, is_selected in enumerate(individual) if is_selected]\n",
    "    X_train_selected = X_train_selected[:, selected_features]\n",
    "    X_test_selected = X_test_selected[:, selected_features]\n",
    "\n",
    "    #classifier = RandomForestClassifier(n_estimators=500, random_state=1)\n",
    "    classifier = RandomForestClassifier(\n",
    "        n_estimators=50,\n",
    "        max_depth=10,\n",
    "        max_features='sqrt',\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    classifier.fit(X_train_selected, y_train_all)\n",
    "    y_pred = classifier.predict(X_test_selected)\n",
    "\n",
    "    accuracy = f1_score(y_test_all, y_pred, average='weighted')  # Use 'micro', 'macro', or 'weighted' as needed\n",
    "\n",
    "    # Calculate the count of ones in the gene (number of selected features)\n",
    "    ones_count = sum(individual)\n",
    "\n",
    "    # Penalize having more ones (features) if penalty_enabled is True\n",
    "    if penalty_enabled:\n",
    "        ones_penalty = ones_count / len(individual)\n",
    "        fitness = accuracy - ones_penalty\n",
    "    else:\n",
    "        fitness = accuracy\n",
    "\n",
    "    return fitness,\n",
    "\n",
    "# Skip the creator creation if already defined\n",
    "try:\n",
    "    creator.FitnessMax\n",
    "except AttributeError:\n",
    "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "\n",
    "# Skip the creator creation if already defined\n",
    "try:\n",
    "    creator.Individual\n",
    "except AttributeError:\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "# Create the DEAP Toolbox\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# Create the attributes and register them with the toolbox\n",
    "n_features = len(X_test_all[0])\n",
    "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=n_features)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Register the evaluation function with the toolbox and pass dataset\n",
    "toolbox.register(\"evaluate\", evaluate, X_train_selected=X_train_all, X_test_selected=X_test_all, y_train_all=y_train_all, y_test_all=y_test_all)\n",
    "\n",
    "# Create a new generation of individuals using tournament selection and one-point crossover\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "# Define the number of generations and population size\n",
    "# n_generations = 10\n",
    "# population_size = 500\n",
    "\n",
    "n_generations = 2\n",
    "population_size = 100\n",
    "\n",
    "\n",
    "# Create an initial population with only two features\n",
    "initial_population = [toolbox.individual() for _ in range(2)]\n",
    "\n",
    "# Evaluate the initial population\n",
    "initial_fitnesses = list(map(toolbox.evaluate, initial_population))\n",
    "for ind, fit in zip(initial_population, initial_fitnesses):\n",
    "    ind.fitness.values = fit\n",
    "\n",
    "# Combine the initial population with the main population\n",
    "population = initial_population + toolbox.population(n=population_size - 2)\n",
    "\n",
    "# Track the best individual across generations\n",
    "best_individual = None\n",
    "best_fitness = float('-inf')\n",
    "\n",
    "# Create a list to store results\n",
    "results = []\n",
    "\n",
    "# Start the evolution process\n",
    "for generation in range(n_generations):\n",
    "    print(f\"Generation {generation + 1}/{n_generations}\")\n",
    "\n",
    "    start_time = time.time()  # Record the start time for the current generation\n",
    "\n",
    "    # Select the next generation of individuals\n",
    "    offspring = toolbox.select(population, len(population))\n",
    "\n",
    "    # Clone the selected individuals\n",
    "    offspring = list(map(toolbox.clone, offspring))\n",
    "\n",
    "    # Apply crossover and mutation to the offspring\n",
    "    for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "        if random.random() < 0.5:\n",
    "            toolbox.mate(child1, child2)\n",
    "            del child1.fitness.values\n",
    "            del child2.fitness.values\n",
    "\n",
    "    # Ensure that no new individuals have all zeros\n",
    "    for mutant in offspring:\n",
    "        if random.random() < 0.2:\n",
    "            toolbox.mutate(mutant)\n",
    "            del mutant.fitness.values\n",
    "\n",
    "            # Check if the new individual has all zeros\n",
    "            if sum(mutant) == 0:\n",
    "                # If it has all zeros, randomly set one gene to 1\n",
    "                random_index = random.randint(0, len(mutant) - 1)\n",
    "                mutant[random_index] = 1\n",
    "\n",
    "    # Evaluate the fitness of the offspring\n",
    "    fitnesses = list(map(toolbox.evaluate, offspring))\n",
    "    for ind, fit in zip(offspring, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    # Replace the old population with the offspring\n",
    "    population[:] = offspring\n",
    "\n",
    "    # Update the best individual and fitness\n",
    "    for ind in population:\n",
    "        if ind.fitness.values[0] > best_fitness:\n",
    "            best_individual = ind\n",
    "            best_fitness = ind.fitness.values[0]\n",
    "\n",
    "    # Print the best fitness value in this generation\n",
    "    print(f\"Best Fitness: {best_fitness:.4f}\")\n",
    "\n",
    "    end_time = time.time()  # Record the end time for the current generation\n",
    "    time_taken = end_time - start_time  # Calculate the time taken for the current generation\n",
    "\n",
    "    # Print the time taken for the current generation\n",
    "    print(f\"Time taken for Generation {generation + 1}: {time_taken:.2f} seconds\")\n",
    "\n",
    "    selected_features_indices = [i for i, is_selected in enumerate(best_individual) if is_selected]\n",
    "    selected_feature_names = [feature_names[i] for i in selected_features_indices]\n",
    "\n",
    "    # Store results for this generation\n",
    "    results.append({'Generation': generation + 1, 'Iteration': generation + 1,\n",
    "                     'Best Fitness': best_fitness, 'Time Taken': time_taken,\n",
    "                     'Feature Len':len(selected_feature_names),'Feature Names':selected_feature_names})\n",
    "\n",
    "    # Check if best_fitness reached 1.0, and stop if true\n",
    "    if best_fitness == 1.0:\n",
    "        print(\"Accuracy reached 1.0. Stopping evolution.\")\n",
    "        break\n",
    "\n",
    "# Print the selected features in the best individual\n",
    "selected_features_indices = [i for i, is_selected in enumerate(best_individual) if is_selected]\n",
    "selected_feature_names = [feature_names[i] for i in selected_features_indices]\n",
    "print(\"Selected features in the best individual:\", selected_feature_names)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results DataFrame\n",
    "print(results_df)\n",
    "\n",
    "# Save the results to an Excel file\n",
    "results_df.to_excel(\"evolution_results.xlsx\", index=False)\n",
    "\n",
    "# Save the best individual to a file\n",
    "best_individual_file_name = \"best_individual.pkl\"\n",
    "with open(best_individual_file_name, \"wb\") as best_individual_file:\n",
    "    pickle.dump(best_individual, best_individual_file)\n",
    "\n",
    "# Save the feature names to a file\n",
    "feature_names_file_name = \"feature_names.pkl\"\n",
    "with open(feature_names_file_name, \"wb\") as feature_names_file:\n",
    "    pickle.dump(feature_names, feature_names_file)\n",
    "\n",
    "# Print final results\n",
    "print(\"Final Best Fitness:\", best_fitness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cv-lOzaLjQX6"
   },
   "outputs": [],
   "source": [
    "best_individual_file_name = \"best_individual.pkl\"\n",
    "feature_names_file_name = \"feature_names.pkl\"\n",
    "\n",
    "# Load the best individual from the file\n",
    "with open(best_individual_file_name, \"rb\") as best_individual_file:\n",
    "    loaded_best_individual = pickle.load(best_individual_file)\n",
    "\n",
    "# Load the feature_names from the file\n",
    "with open(feature_names_file_name, \"rb\") as feature_names_file:\n",
    "    feature_names = pickle.load(feature_names_file)\n",
    "\n",
    "# Test the loaded best individual on a random instance from the testing data\n",
    "random_instance_index = random.randint(0, len(X_test_all) - 1)\n",
    "X_random_instance = X_test_all[random_instance_index, :]\n",
    "y_random_instance = y_test_all[random_instance_index]\n",
    "\n",
    "selected_features_indices = [i for i, is_selected in enumerate(loaded_best_individual) if is_selected]\n",
    "selected_feature_names = [feature_names[i] for i in selected_features_indices]\n",
    "X_random_instance_selected = X_random_instance[selected_features_indices]\n",
    "\n",
    "# classifier = RandomForestClassifier(n_estimators=500, random_state=1)\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    max_features='sqrt',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "classifier.fit(X_train_all[:, selected_features_indices], y_train_all)\n",
    "\n",
    "\n",
    "y_pred_random_instance = classifier.predict([X_random_instance_selected])\n",
    "# Define class names\n",
    "class_names = ['benign', 'iodine', 'dns2tcp', 'dnscat2']\n",
    "\n",
    "# Print the actual instance label and predicted instance label with class names\n",
    "print(f\"Actual instance:    ({y_random_instance})-{class_names[y_random_instance]}\")\n",
    "print(f\"Predicted instance: ({y_pred_random_instance[0]})-{class_names[y_pred_random_instance[0]]}\")\n",
    "print(\"Selected features in the best individual:\", selected_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRQJoeLVjQX7"
   },
   "outputs": [],
   "source": [
    "penalty_enabled=True\n",
    "\n",
    "# Function to evaluate an individual's fitness\n",
    "def evaluate(individual, X_train_selected, X_test_selected, y_train_all, y_test_all, penalty_enabled=True):\n",
    "    selected_features = [i for i, is_selected in enumerate(individual) if is_selected]\n",
    "    X_train_selected = X_train_selected[:, selected_features]\n",
    "    X_test_selected = X_test_selected[:, selected_features]\n",
    "\n",
    "    # classifier = RandomForestClassifier(n_estimators=500, random_state=1)\n",
    "    classifier = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    max_features='sqrt',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "    classifier.fit(X_train_selected, y_train_all)\n",
    "    y_pred = classifier.predict(X_test_selected)\n",
    "\n",
    "    accuracy = f1_score(y_test_all, y_pred, average='weighted')  # Use 'micro', 'macro', or 'weighted' as needed\n",
    "\n",
    "    # Calculate the count of ones in the gene (number of selected features)\n",
    "    ones_count = sum(individual)\n",
    "\n",
    "    # Penalize having more ones (features) if penalty_enabled is True\n",
    "    if penalty_enabled:\n",
    "        ones_penalty = ones_count / len(individual)\n",
    "        fitness = accuracy - ones_penalty\n",
    "    else:\n",
    "        fitness = accuracy\n",
    "\n",
    "    return fitness,\n",
    "\n",
    "# Skip the creator creation if already defined\n",
    "try:\n",
    "    creator.FitnessMax\n",
    "except AttributeError:\n",
    "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "\n",
    "# Skip the creator creation if already defined\n",
    "try:\n",
    "    creator.Individual\n",
    "except AttributeError:\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "# Create the DEAP Toolbox\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# Create the attributes and register them with the toolbox\n",
    "n_features = len(X_test_all[0])\n",
    "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=n_features)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Register the evaluation function with the toolbox and pass dataset\n",
    "toolbox.register(\"evaluate\", evaluate, X_train_selected=X_train_all, X_test_selected=X_test_all, y_train_all=y_train_all, y_test_all=y_test_all)\n",
    "\n",
    "# Create a new generation of individuals using tournament selection and one-point crossover\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "# Define the number of generations and population size\n",
    "n_generations = 2\n",
    "population_size = 100\n",
    "\n",
    "# Create an initial population with only two features\n",
    "initial_population = [toolbox.individual() for _ in range(2)]\n",
    "\n",
    "# Evaluate the initial population\n",
    "initial_fitnesses = list(map(toolbox.evaluate, initial_population))\n",
    "for ind, fit in zip(initial_population, initial_fitnesses):\n",
    "    ind.fitness.values = fit\n",
    "\n",
    "# Combine the initial population with the main population\n",
    "population = initial_population + toolbox.population(n=population_size - 2)\n",
    "\n",
    "# Track the best individual across generations\n",
    "best_individual = None\n",
    "best_fitness = float('-inf')\n",
    "\n",
    "# Create a list to store results\n",
    "results = []\n",
    "\n",
    "# Start the evolution process\n",
    "for generation in range(n_generations):\n",
    "    print(f\"Generation {generation + 1}/{n_generations}\")\n",
    "\n",
    "    start_time = time.time()  # Record the start time for the current generation\n",
    "\n",
    "    # Select the next generation of individuals\n",
    "    offspring = toolbox.select(population, len(population))\n",
    "\n",
    "    # Clone the selected individuals\n",
    "    offspring = list(map(toolbox.clone, offspring))\n",
    "\n",
    "    # Apply crossover and mutation to the offspring\n",
    "    for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "        if random.random() < 0.5:\n",
    "            toolbox.mate(child1, child2)\n",
    "            del child1.fitness.values\n",
    "            del child2.fitness.values\n",
    "\n",
    "    # Ensure that no new individuals have all zeros\n",
    "    for mutant in offspring:\n",
    "        if random.random() < 0.2:\n",
    "            toolbox.mutate(mutant)\n",
    "            del mutant.fitness.values\n",
    "\n",
    "            # Check if the new individual has all zeros\n",
    "            if sum(mutant) == 0:\n",
    "                # If it has all zeros, randomly set one gene to 1\n",
    "                random_index = random.randint(0, len(mutant) - 1)\n",
    "                mutant[random_index] = 1\n",
    "\n",
    "    # Evaluate the fitness of the offspring\n",
    "    fitnesses = list(map(toolbox.evaluate, offspring))\n",
    "    for ind, fit in zip(offspring, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    # Replace the old population with the offspring\n",
    "    population[:] = offspring\n",
    "\n",
    "    # Update the best individual and fitness\n",
    "    for ind in population:\n",
    "        if ind.fitness.values[0] > best_fitness:\n",
    "            best_individual = ind\n",
    "            best_fitness = ind.fitness.values[0]\n",
    "\n",
    "    # Print the best fitness value in this generation\n",
    "    print(f\"Best Fitness: {best_fitness:.4f}\")\n",
    "\n",
    "    end_time = time.time()  # Record the end time for the current generation\n",
    "    time_taken = end_time - start_time  # Calculate the time taken for the current generation\n",
    "\n",
    "    # Print the time taken for the current generation\n",
    "    print(f\"Time taken for Generation {generation + 1}: {time_taken:.2f} seconds\")\n",
    "\n",
    "    selected_features_indices = [i for i, is_selected in enumerate(best_individual) if is_selected]\n",
    "    selected_feature_names = [feature_names[i] for i in selected_features_indices]\n",
    "\n",
    "    # Store results for this generation\n",
    "    results.append({'Generation': generation + 1, 'Iteration': generation + 1,\n",
    "                     'Best Fitness': best_fitness, 'Time Taken': time_taken,\n",
    "                     'Feature Len':len(selected_feature_names),'Feature Names':selected_feature_names})\n",
    "\n",
    "    # Check if best_fitness reached 1.0, and stop if true\n",
    "    if best_fitness == 1.0:\n",
    "        print(\"Accuracy reached 1.0. Stopping evolution.\")\n",
    "        break\n",
    "\n",
    "# Print the selected features in the best individual\n",
    "selected_features_indices = [i for i, is_selected in enumerate(best_individual) if is_selected]\n",
    "selected_feature_names = [feature_names[i] for i in selected_features_indices]\n",
    "print(\"Selected features in the best individual:\", selected_feature_names)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results DataFrame\n",
    "print(results_df)\n",
    "\n",
    "# Save the results to an Excel file\n",
    "results_df.to_excel(\"evolution_results.xlsx\", index=False)\n",
    "\n",
    "# Save the best individual to a file\n",
    "best_individual_file_name = \"best_individual.pkl\"\n",
    "with open(best_individual_file_name, \"wb\") as best_individual_file:\n",
    "    pickle.dump(best_individual, best_individual_file)\n",
    "\n",
    "# Save the feature names to a file\n",
    "feature_names_file_name = \"feature_names.pkl\"\n",
    "with open(feature_names_file_name, \"wb\") as feature_names_file:\n",
    "    pickle.dump(feature_names, feature_names_file)\n",
    "\n",
    "# Print final results\n",
    "print(\"Final Best Fitness:\", best_fitness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GbMYz8ZNjQX8"
   },
   "outputs": [],
   "source": [
    "best_individual_file_name = \"best_individual.pkl\"\n",
    "feature_names_file_name = \"feature_names.pkl\"\n",
    "\n",
    "# Load the best individual from the file\n",
    "with open(best_individual_file_name, \"rb\") as best_individual_file:\n",
    "    loaded_best_individual = pickle.load(best_individual_file)\n",
    "\n",
    "# Load the feature_names from the file\n",
    "with open(feature_names_file_name, \"rb\") as feature_names_file:\n",
    "    feature_names = pickle.load(feature_names_file)\n",
    "\n",
    "# Test the loaded best individual on a random instance from the testing data\n",
    "random_instance_index = random.randint(0, len(X_test_all) - 1)\n",
    "X_random_instance = X_test_all[random_instance_index, :]\n",
    "y_random_instance = y_test_all[random_instance_index]\n",
    "\n",
    "selected_features_indices = [i for i, is_selected in enumerate(loaded_best_individual) if is_selected]\n",
    "selected_feature_names = [feature_names[i] for i in selected_features_indices]\n",
    "X_random_instance_selected = X_random_instance[selected_features_indices]\n",
    "\n",
    "# classifier = RandomForestClassifier(n_estimators=500, random_state=1)\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    max_features='sqrt',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "classifier.fit(X_train_all[:, selected_features_indices], y_train_all)\n",
    "\n",
    "\n",
    "y_pred_random_instance = classifier.predict([X_random_instance_selected])\n",
    "# Define class names\n",
    "class_names = ['benign', 'iodine', 'dns2tcp', 'dnscat2']\n",
    "\n",
    "# Print the actual instance label and predicted instance label with class names\n",
    "print(f\"Actual instance:    ({y_random_instance})-{class_names[y_random_instance]}\")\n",
    "print(f\"Predicted instance: ({y_pred_random_instance[0]})-{class_names[y_pred_random_instance[0]]}\")\n",
    "print(\"Selected features in the best individual:\", selected_feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_zD5dU-kFN6"
   },
   "source": [
    "# Comparing Standard GA and Enhanced GA on Benchmark Functions\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project compares the **Standard Genetic Algorithm (GA)** with an **Enhanced Genetic Algorithm (Enhanced GA)**. The Enhanced GA includes penalties to improve optimization performance. These algorithms are tested on common mathematical benchmark functions.\n",
    "\n",
    "---\n",
    "\n",
    "## Benchmark Functions\n",
    "\n",
    "### 1. Sphere Function (Unimodal)\n",
    "$$\n",
    "f(x) = \\sum_{i=1}^n x_i^2\n",
    "$$\n",
    "- **Type**: Unimodal  \n",
    "- **Domain**: \\( x_i \\in [-5.12, 5.12] \\)  \n",
    "- **Global Minimum**: \\( f(0, 0, \\dots, 0) = 0 \\)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Rastrigin Function (Multimodal)\n",
    "$$\n",
    "f(x) = 10n + \\sum_{i=1}^n \\left( x_i^2 - 10\\cos(2\\pi x_i) \\right)\n",
    "$$\n",
    "- **Type**: Multimodal  \n",
    "- **Domain**: \\( x_i \\in [-5.12, 5.12] \\)  \n",
    "- **Global Minimum**: \\( f(0, 0, \\dots, 0) = 0 \\)\n",
    "\n",
    "---\n",
    "\n",
    "## Genetic Algorithm Overview\n",
    "\n",
    "### Standard GA\n",
    "A standard genetic algorithm follows these steps:\n",
    "1. **Initialization**: Randomly generate a population of solutions.\n",
    "2. **Selection**: Select individuals based on fitness.\n",
    "3. **Crossover**: Combine two individuals to create offspring.\n",
    "4. **Mutation**: Introduce random changes to maintain diversity.\n",
    "5. **Evaluation**: Evaluate the fitness of offspring.\n",
    "6. **Iteration**: Repeat the above steps for several generations.\n",
    "\n",
    "### Enhanced GA\n",
    "The Enhanced GA modifies the standard GA by:\n",
    "1. **Penalty Mechanism**: Adds a penalty to fitness based on undesirable traits (e.g., large values).\n",
    "2. **Dynamic Mutation**: Adjusts mutation probability during evolution.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 9630,
     "status": "ok",
     "timestamp": 1737290486291,
     "user": {
      "displayName": "Mahmoud Sammour",
      "userId": "07606788054442128619"
     },
     "user_tz": -120
    },
    "id": "4QOng7YWkFpX",
    "outputId": "e253e11c-8538-431e-9c75-f834a844bebd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deap\n",
      "  Downloading deap-1.4.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from deap) (1.26.4)\n",
      "Downloading deap-1.4.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: deap\n",
      "Successfully installed deap-1.4.2\n",
      "Running Standard GA on Sphere...\n",
      "Running Enhanced GA on Sphere...\n",
      "Running Standard GA on Rastrigin...\n",
      "Running Enhanced GA on Rastrigin...\n",
      "\n",
      "Best Individual (Standard GA - Sphere): [0.017717713309582255, 0.006752983026720055, -0.008911605440491535, -0.0019591881721735547, 0.002516347935905736, -0.008050789057978227, -0.002385153912009599, 0.014085414529486067, 0.009649104007259988, 0.0035597415531461356]\n",
      "Best Individual (Enhanced GA - Sphere): [0.0010301783324623164, -0.0007391449540424463, -0.0017994391037613527, 0.0026063245844631672, 0.0014142348390273813, -0.004407240401536236, 0.0013992712689238952, -0.0013593785727692854, -0.005806555944730822, -0.0007720034429961302]\n",
      "\n",
      "Best Individual (Standard GA - Rastrigin): [-0.9780721327436117, 0.00571461491635867, -1.01016388440623, 1.019232280878383, 0.9678573610650738, 0.02351835122952653, 0.9850622641485658, -0.018351526798426335, -2.0187112545952495, 0.9994919023156641]\n",
      "Best Individual (Enhanced GA - Rastrigin): [0.0018245665263798855, -0.016153895938172576, -0.019595342842307302, -0.9803640309223016, 0.02025185971476062, -0.0544489046620501, 1.9776957790311493, -0.019056325607560935, 0.018561843259348537, 0.9882251233652363]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"500ed890-bec0-42f2-97b4-aadc57035cea\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"500ed890-bec0-42f2-97b4-aadc57035cea\")) {                    Plotly.newPlot(                        \"500ed890-bec0-42f2-97b4-aadc57035cea\",                        [{\"mode\":\"lines+markers\",\"name\":\"Standard GA - Sphere\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50],\"y\":[23.846241046985934,23.846241046985934,21.083706701199095,14.246129988379971,8.699776236808308,7.642375444835446,6.3825569054390145,3.564496834263315,2.5102163800626918,2.426965723619034,2.028417812695764,1.3294161932340804,1.1435912906826429,1.1600395920362392,1.026656471557817,0.8396115043445075,0.6637359655720593,0.30502858078021733,0.30502858078021733,0.2941436675153336,0.1620979010283256,0.23809932600592834,0.17034366409840407,0.14860085022323422,0.11974041930898487,0.12991495753017124,0.1279114207690286,0.10202379692113135,0.06949825172479183,0.05746092313376745,0.045866218936364916,0.039617671211708824,0.036431009596563986,0.029805717134067863,0.03317561381806058,0.03146504631823626,0.02400161274860419,0.022340533807108334,0.010738263271872373,0.00945611421824144,0.00945611421824144,0.006371265986155451,0.005120853808260169,0.005439112063032177,0.004025747346158853,0.003452289242717019,0.0023159759820405874,0.0022831023636303727,0.0012692014557783829,0.0008237873156108473,0.0008237873156108473],\"type\":\"scatter\"},{\"mode\":\"lines+markers\",\"name\":\"Enhanced GA - Sphere\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50],\"y\":[26.925702829930977,22.938882655982468,6.762541507672821,6.762541507672821,6.762541507672821,6.281863511575099,6.175861151999773,4.119316677649935,2.688369490930969,1.7090014198570298,1.3133420458483689,1.3133420458483689,0.9700144386382344,0.48360257787941935,0.755486936246196,0.593027875063095,0.3921412314139583,0.33869712592859197,0.17845674630087088,0.1976431100009615,0.15493585926829911,0.15562418951431894,0.11266417123411016,0.07663343940433397,0.07663343940433397,0.0601684483948717,0.0601684483948717,0.04654304385099231,0.039363739245522805,0.03751017696504187,0.03272949043520231,0.02031632855703821,0.01724471634840491,0.012097015436856744,0.012097015436856744,0.012097015436856744,0.010350106960522236,0.010350106960522236,0.009816304989537167,0.008766681780029859,0.008439253177039745,0.007012271916753671,0.006477175854594724,0.005865079691988382,0.004533672520070081,0.004034136806926351,0.003874818373513559,0.0029591210575630415,0.0028488427893585393,0.002581873658555444,0.002204557435639066],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Fitness Convergence Over Generations\"},\"xaxis\":{\"title\":{\"text\":\"Generation\"}},\"yaxis\":{\"title\":{\"text\":\"Fitness (Lower is Better)\"}},\"legend\":{\"title\":{\"text\":\"Algorithm\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('500ed890-bec0-42f2-97b4-aadc57035cea');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"e41f6c0b-5e88-4619-ac61-f07467a0fe7e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e41f6c0b-5e88-4619-ac61-f07467a0fe7e\")) {                    Plotly.newPlot(                        \"e41f6c0b-5e88-4619-ac61-f07467a0fe7e\",                        [{\"mode\":\"lines+markers\",\"name\":\"Standard GA - Rastrigin\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50],\"y\":[96.8673895108336,91.93335318004635,84.80652239205286,73.67119544561123,69.00938199704498,61.33498521356442,61.33498521356442,65.63840749360561,59.94226824709929,58.95406494289887,59.64575756346525,59.64575756346525,50.40296410371876,52.03921912161878,52.03921912161878,54.91790201592037,51.56198019653972,50.05329724053534,34.94313349216998,44.32736976716602,49.80505591029681,42.135854660526306,34.12350364320423,34.12350364320423,34.12350364320423,22.608746396404186,22.608746396404186,22.608746396404186,22.608746396404186,22.608746396404186,24.891772686334804,26.444805832791133,24.06011849042163,24.06011849042163,22.69432563134741,22.69432563134741,15.759087603357884,15.759087603357884,15.759087603357884,15.759087603357884,13.697653759895232,15.759087603357884,16.654514884044445,15.467832995592516,15.556848275014303,14.987450775175105,12.64355370114545,11.525859548979952,10.688616956962107,10.688616956962107,10.684325064408867],\"type\":\"scatter\"},{\"mode\":\"lines+markers\",\"name\":\"Enhanced GA - Rastrigin\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50],\"y\":[108.23402025629268,98.2049538942965,98.2049538942965,93.05517299771742,96.49866967399717,90.62940124063373,90.02869254620113,66.78827657370616,66.78827657370616,77.48735408615202,65.0239761356224,56.22873419922368,56.22873419922368,44.89410350417197,44.89410350417197,44.89410350417197,52.325798504723885,52.325798504723885,37.18278086719945,37.18278086719945,40.583234965083534,45.38914190734149,39.86175970243454,32.10536708074561,31.17520811400437,31.17520811400437,35.40389607652456,26.182482279572742,25.15055717099989,16.04064918135354,19.591978516787126,19.591978516787126,20.565651839025723,20.565651839025723,20.178510986346673,12.907899711801738,14.884445643491365,15.511889956843557,15.612372488693591,12.574351435392838,10.631941324435488,10.631941324435488,9.076270925406071,9.076270925406071,9.076270925406071,8.832902717502497,8.600748426334103,8.285400404021967,8.104589204333156,7.392454644473201,7.392454644473201],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Fitness Convergence Over Generations\"},\"xaxis\":{\"title\":{\"text\":\"Generation\"}},\"yaxis\":{\"title\":{\"text\":\"Fitness (Lower is Better)\"}},\"legend\":{\"title\":{\"text\":\"Algorithm\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('e41f6c0b-5e88-4619-ac61-f07467a0fe7e');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install deap\n",
    "import random\n",
    "import numpy as np\n",
    "from deap import base, creator, tools, algorithms\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Define Benchmark Functions\n",
    "def sphere(individual):\n",
    "    \"\"\"Sphere Function: f(x) = sum(x_i^2)\"\"\"\n",
    "    return sum(x**2 for x in individual),\n",
    "\n",
    "def rastrigin(individual):\n",
    "    \"\"\"Rastrigin Function: f(x) = 10n + sum(x_i^2 - 10*cos(2*pi*x_i))\"\"\"\n",
    "    return 10 * len(individual) + sum(x**2 - 10 * np.cos(2 * np.pi * x) for x in individual),\n",
    "\n",
    "# Setup Genetic Algorithm (GA)\n",
    "def setup_ga(bounds, n_dimensions):\n",
    "    \"\"\"Sets up the Genetic Algorithm.\"\"\"\n",
    "    lower_bound, upper_bound = bounds\n",
    "\n",
    "    # Skip re-creating classes if they are already defined\n",
    "    if not hasattr(creator, \"FitnessMin\"):\n",
    "        creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))  # Minimize the function\n",
    "    if not hasattr(creator, \"Individual\"):\n",
    "        creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "    # Toolbox Registration\n",
    "    toolbox = base.Toolbox()\n",
    "    toolbox.register(\"attr_float\", random.uniform, lower_bound, upper_bound)\n",
    "    toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, n=n_dimensions)\n",
    "    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "    toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
    "    toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=1, indpb=0.2)\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "    return toolbox\n",
    "\n",
    "# Run Standard GA\n",
    "def run_standard_ga(toolbox, evaluate_func, n_population, n_generations):\n",
    "    \"\"\"Runs the Standard Genetic Algorithm.\"\"\"\n",
    "    toolbox.register(\"evaluate\", evaluate_func)\n",
    "    population = toolbox.population(n=n_population)\n",
    "    hof = tools.HallOfFame(1)  # Track the best solution\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "\n",
    "    population, logbook = algorithms.eaSimple(\n",
    "        population, toolbox, cxpb=0.7, mutpb=0.2, ngen=n_generations,\n",
    "        stats=stats, halloffame=hof, verbose=False\n",
    "    )\n",
    "\n",
    "    return hof[0], logbook\n",
    "\n",
    "# Run Enhanced GA\n",
    "def run_enhanced_ga(toolbox, evaluate_func, penalty_func, n_population, n_generations):\n",
    "    \"\"\"Runs the Enhanced Genetic Algorithm with penalties.\"\"\"\n",
    "    def evaluate_with_penalty(ind):\n",
    "        base_fitness = evaluate_func(ind)[0]\n",
    "        penalty = penalty_func(ind)\n",
    "        return base_fitness + penalty,\n",
    "\n",
    "    toolbox.register(\"evaluate\", evaluate_with_penalty)\n",
    "    population = toolbox.population(n=n_population)\n",
    "    hof = tools.HallOfFame(1)  # Track the best solution\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "\n",
    "    population, logbook = algorithms.eaSimple(\n",
    "        population, toolbox, cxpb=0.7, mutpb=0.2, ngen=n_generations,\n",
    "        stats=stats, halloffame=hof, verbose=False\n",
    "    )\n",
    "\n",
    "    return hof[0], logbook\n",
    "\n",
    "# Define Penalty Functions\n",
    "def simple_penalty(individual):\n",
    "    \"\"\"Penalizes large values.\"\"\"\n",
    "    return sum(abs(x) for x in individual) / len(individual)\n",
    "\n",
    "# Visualization using Plotly\n",
    "def plot_convergence(logbooks, labels):\n",
    "    \"\"\"Plots the convergence of fitness values using Plotly.\"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for logbook, label in zip(logbooks, labels):\n",
    "        gen = logbook.select(\"gen\")\n",
    "        fit_mins = logbook.select(\"min\")\n",
    "        fig.add_trace(go.Scatter(x=gen, y=fit_mins, mode='lines+markers', name=label))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Fitness Convergence Over Generations\",\n",
    "        xaxis_title=\"Generation\",\n",
    "        yaxis_title=\"Fitness (Lower is Better)\",\n",
    "        legend_title=\"Algorithm\",\n",
    "        template=\"plotly\"\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# Experiment Parameters\n",
    "n_dimensions = 10\n",
    "bounds = (-5.12, 5.12)  # Range for Rastrigin and Sphere functions\n",
    "n_population = 100\n",
    "n_generations = 50\n",
    "\n",
    "# Initialize GA\n",
    "toolbox = setup_ga(bounds, n_dimensions)\n",
    "\n",
    "# Run Experiments\n",
    "print(\"Running Standard GA on Sphere...\")\n",
    "best_standard_sphere, log_standard_sphere = run_standard_ga(toolbox, sphere, n_population, n_generations)\n",
    "\n",
    "print(\"Running Enhanced GA on Sphere...\")\n",
    "best_enhanced_sphere, log_enhanced_sphere = run_enhanced_ga(toolbox, sphere, simple_penalty, n_population, n_generations)\n",
    "\n",
    "print(\"Running Standard GA on Rastrigin...\")\n",
    "best_standard_rastrigin, log_standard_rastrigin = run_standard_ga(toolbox, rastrigin, n_population, n_generations)\n",
    "\n",
    "print(\"Running Enhanced GA on Rastrigin...\")\n",
    "best_enhanced_rastrigin, log_enhanced_rastrigin = run_enhanced_ga(toolbox, rastrigin, simple_penalty, n_population, n_generations)\n",
    "\n",
    "# Results\n",
    "print(\"\\nBest Individual (Standard GA - Sphere):\", best_standard_sphere)\n",
    "print(\"Best Individual (Enhanced GA - Sphere):\", best_enhanced_sphere)\n",
    "\n",
    "print(\"\\nBest Individual (Standard GA - Rastrigin):\", best_standard_rastrigin)\n",
    "print(\"Best Individual (Enhanced GA - Rastrigin):\", best_enhanced_rastrigin)\n",
    "\n",
    "# Plot Convergence\n",
    "plot_convergence(\n",
    "    [log_standard_sphere, log_enhanced_sphere],\n",
    "    [\"Standard GA - Sphere\", \"Enhanced GA - Sphere\"]\n",
    ")\n",
    "\n",
    "plot_convergence(\n",
    "    [log_standard_rastrigin, log_enhanced_rastrigin],\n",
    "    [\"Standard GA - Rastrigin\", \"Enhanced GA - Rastrigin\"]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1aKGMe24q-3zesXzsDR24ekqUnAwftg0u",
     "timestamp": 1692082397536
    },
    {
     "file_id": "1KGM_pWvnkfwxSetE0KOdhbnIrN1zkkky",
     "timestamp": 1691224448663
    }
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
