{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ve2hwdLRokfe"
   },
   "source": [
    "# Enhanced detection of DNS tunnelling: Leveraging random forest and genetic algorithm for improved security"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xr9XZvver6gp"
   },
   "source": [
    "To download the necessary CSV files for the project, use the following `wget` commands:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLL6gQY85FFF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# List of file URLs to download\n",
    "file_urls = [\n",
    "    (\"https://drive.usercontent.google.com/download?id=1cictwnxUyu1vCa4H9iefIrQeVLCC3RCv&export=download&authuser=0&confirm=t&uuid=8ec5d698-4d5d-4592-94eb-8a82234966ac&at=AC2mKKTzwehwnBUepaEJIDoKDql-:1690876827674\", \"benign-chrome.csv\"),\n",
    "    (\"https://drive.usercontent.google.com/download?id=1cms99qEylyvesqcX3dQRZOUQRAONy2uS&export=download&authuser=0&confirm=t&uuid=0f089685-41f1-40fe-903e-8fcc8e2bcac8&at=AC2mKKSfqH9g0sjW4mQVa5-J4gMf:1690877149684\", \"benign-firefox.csv\"),\n",
    "    (\"https://drive.usercontent.google.com/download?id=1cqDL7A_kdOCL4Km4uUifRPllFmB3WaZ_&export=download&authuser=0&confirm=t&uuid=19171c97-ad00-4af4-bf46-ef8c453b2964&at=AC2mKKROICucTfu1coxAIff16wi1:1690878058234\", \"mal-dns2tcp.csv\"),\n",
    "    (\"https://drive.usercontent.google.com/download?id=1cxeTvXNV-OY_4T6xs4sUB98lmanROw3m&export=download&authuser=0&confirm=t&uuid=67df7c64-15ed-450d-bad8-f416080d378d&at=AC2mKKST9kQGoFcvwe9EhJoY6jRA:1690878087508\", \"mal-dnscat2.csv\"),\n",
    "    (\"https://drive.google.com/u/1/uc?id=1czNRMpNyicFNYW2fbK_WjsoF77qB9_XA&export=download\", \"mal-iodine.csv\")\n",
    "]\n",
    "\n",
    "# Create a directory to save the files\n",
    "if not os.path.exists(\"DoHBrw-2020\"):\n",
    "    os.makedirs(\"DoHBrw-2020\")\n",
    "\n",
    "# Loop through the file URLs and download files if not already present\n",
    "for url, filename in file_urls:\n",
    "    file_path = os.path.join(\"DoHBrw-2020\", filename)\n",
    "    if not os.path.exists(file_path):\n",
    "        try:\n",
    "            print(f\"Downloading {filename}...\")\n",
    "            response = requests.get(url, stream=True)\n",
    "            with open(file_path, \"wb\") as file:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    file.write(chunk)\n",
    "            print(f\"{filename} downloaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {filename}: {e}\")\n",
    "    else:\n",
    "        print(f\"{filename} already exists!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vs6ZiJsAosz4"
   },
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import json  # For working with JSON data\n",
    "import math  # For mathematical operations\n",
    "from collections import Counter  # For counting elements in a list\n",
    "from os.path import join  # For joining file paths\n",
    "import numpy as np  # For numerical operations and arrays\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import plotly.express as px  # For interactive plotting\n",
    "import plotly.figure_factory as ff  # For creating various types of figures\n",
    "import plotly.graph_objects as go  # For creating customized plots\n",
    "import random  # For generating random values\n",
    "from tqdm.notebook import tqdm, trange  # For displaying progress bars in Jupyter Notebook\n",
    "from deap import base, creator, tools, algorithms  # For evolutionary algorithms\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, f1_score  # For model evaluation metrics\n",
    "from sklearn.model_selection import cross_val_score, train_test_split  # For cross-validation and data splitting\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder  # For data preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier  # For building a Random Forest classifier\n",
    "from sklearn.impute import SimpleImputer  # For imputing missing values\n",
    "from sklearn.inspection import permutation_importance  # For feature importance analysis\n",
    "\n",
    "from plotly.offline import iplot  # For offline plotting\n",
    "\n",
    "# Additional imports\n",
    "import matplotlib.pyplot as plt  # For creating traditional plots\n",
    "\n",
    "# Shuffle data\n",
    "from sklearn.utils import shuffle  # For shuffling data\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import pickle  # Add this import statement\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "noSrJ0VCouHL"
   },
   "outputs": [],
   "source": [
    "# Read the first benign CSV file into a DataFrame called df1_benign\n",
    "df1_benign = pd.read_csv('DoHBrw-2020/benign-chrome.csv', delimiter=',')\n",
    "\n",
    "# Read the second benign CSV file into another DataFrame called df2_benign\n",
    "df2_benign = pd.read_csv('DoHBrw-2020/benign-firefox.csv', delimiter=',')\n",
    "\n",
    "# Append the contents of df2_benign to df1_benign (Note: This does not modify df1_benign in-place, it returns a new DataFrame)\n",
    "df_benign = pd.concat([df1_benign, df2_benign], ignore_index=True)\n",
    "\n",
    "# Add a new column 'DoH' to df1_benign and set all values in that column to 0, indicating benign traffic\n",
    "df_benign['DoH'] = 0  # 'DoH' stands for DNS-over-HTTPS, and 0 indicates benign traffic\n",
    "\n",
    "# Rename the column 'DoH' to 'labels' in df1_benign\n",
    "df_benign = df_benign.rename(columns={'DoH': 'labels'})\n",
    "df_benign\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTuUD1wKovWS"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read the first malicious CSV file into a DataFrame called df1_malic\n",
    "df1_malic = pd.read_csv('DoHBrw-2020/mal-iodine.csv', delimiter=',')\n",
    "\n",
    "# Add a new column 'DoH' to df1_malic and set all values in that column to 1, indicating malicious traffic of type 'iodine'\n",
    "df1_malic['DoH'] = 1  # 1 stands for 'iodine' (a type of malicious traffic)\n",
    "\n",
    "# Read the second malicious CSV file into another DataFrame called df2_malic\n",
    "df2_malic = pd.read_csv('DoHBrw-2020/mal-dns2tcp.csv', delimiter=',')\n",
    "\n",
    "# Add a new column 'DoH' to df2_malic and set all values in that column to 2, indicating malicious traffic of type 'dns2tcp'\n",
    "df2_malic['DoH'] = 2  # 2 stands for 'dns2tcp' (another type of malicious traffic)\n",
    "\n",
    "# Read the third malicious CSV file into another DataFrame called df3_malic\n",
    "df3_malic = pd.read_csv('DoHBrw-2020/mal-dnscat2.csv', delimiter=',')\n",
    "\n",
    "# Add a new column 'DoH' to df3_malic and set all values in that column to 3, indicating malicious traffic of type 'dnscat2'\n",
    "df3_malic['DoH'] = 3  # 3 stands for 'dnscat2' (yet another type of malicious traffic)\n",
    "\n",
    "# Concatenate the DataFrames df1_malic, df2_malic, and df3_malic into a single DataFrame\n",
    "# The 'ignore_index=True' ensures that the index is reset after concatenation to avoid index duplication\n",
    "df1_malic = pd.concat([df1_malic, df2_malic, df3_malic], ignore_index=True)\n",
    "\n",
    "# Rename the column 'DoH' to 'labels' in df1_malic to have a common label indicating the type of traffic (0 for benign, 1, 2, 3 for malicious types)\n",
    "df1_malic = df1_malic.rename(columns={'DoH': 'labels'})\n",
    "df1_malic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GROAVHRowte"
   },
   "outputs": [],
   "source": [
    "# Shuffle the DataFrame\n",
    "data = shuffle(pd.concat([df_benign, df1_malic], ignore_index=True))\n",
    "\n",
    "# Check the number of null (missing) values in each column of the DataFrame 'data'\n",
    "null_value_counts = data.isnull().sum()\n",
    "\n",
    "# Drop columns with the same value across all rows\n",
    "columns_to_drop = [col for col in data.columns if data[col].nunique() == 1]\n",
    "data_dropped = data.drop(columns=columns_to_drop)\n",
    "\n",
    "# Fill missing values or NaN values with 0 for all columns\n",
    "data_filled = data_dropped.fillna(0)\n",
    "\n",
    "# Print the number of null values after filling\n",
    "print(\"Null Value Counts after Filling:\")\n",
    "print(data_filled.isnull().sum())\n",
    "\n",
    "# Now 'data_filled' contains the DataFrame with missing values filled with 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Icdw-9Xjw8fL"
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tHprQBWCoy15"
   },
   "outputs": [],
   "source": [
    "# Compute the statistical summary of numeric columns in the DataFrame 'data'\n",
    "data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-pFuASzo1YI"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The code data['SourceIP'] is used to access the 'SourceIP' column in the DataFrame data.\n",
    "It retrieves the values of the 'SourceIP' column, which represents the source IP addresses of\n",
    "the network traffic data.\n",
    "\"\"\"\n",
    "data['SourceIP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4vt_x17o0E6"
   },
   "outputs": [],
   "source": [
    "# Compute the count of each unique value in the 'labels' column of the DataFrame 'data'\n",
    "data.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0cG_Ym61HEo-"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Map the numeric labels to their corresponding descriptions\n",
    "attack_descriptions = {\n",
    "    0: \"Benign\",\n",
    "    1: \"Malicious - Iodine\",\n",
    "    2: \"Malicious - DNS2TCP\",\n",
    "    3: \"Malicious - Dnscat2\",\n",
    "}\n",
    "\n",
    "# Convert the 'TimeStamp' column to datetime if it's not already in datetime format\n",
    "data['TimeStamp'] = pd.to_datetime(data['TimeStamp'])\n",
    "\n",
    "# Group the data by 'TimeStamp' and 'labels' to get the count of each attack type at each timestamp\n",
    "grouped_data = data.groupby(['TimeStamp', 'labels']).size().reset_index(name='count')\n",
    "\n",
    "# Create a new column 'AttackTypeDescription' by mapping the 'labels' to their corresponding descriptions\n",
    "grouped_data['AttackTypeDescription'] = grouped_data['labels'].map(attack_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542,
     "output_embedded_package_id": "1ZJTPdRiDRkZTbQYLFddHSj3M0WCSiFoQ"
    },
    "executionInfo": {
     "elapsed": 9246,
     "status": "ok",
     "timestamp": 1692083022512,
     "user": {
      "displayName": "Mahmoud Sammour",
      "userId": "07606788054442128619"
     },
     "user_tz": -180
    },
    "id": "bGEiMNlAyrvB",
    "outputId": "16ed4046-e6c6-4bd1-e84c-71513db42f96"
   },
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "fig = px.line(\n",
    "    grouped_data,\n",
    "    x='TimeStamp',\n",
    "    y='count',\n",
    "    color='AttackTypeDescription',\n",
    "    markers=True,\n",
    "    hover_data={'AttackTypeDescription': True},  # Show attack descriptions on hover\n",
    ")\n",
    "\n",
    "# Update the layout for better readability (optional)\n",
    "fig.update_layout(\n",
    "    title='Attack Type Distribution Over Time',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='Count',\n",
    "    legend_title='Attack Type',\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxHxTvzio5gY"
   },
   "outputs": [],
   "source": [
    "# Create an instance of LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Iterate over all columns in the DataFrame\n",
    "for column in data.columns:\n",
    "    # Check if the column is non-numeric (categorical)\n",
    "    if data[column].dtype == 'object':\n",
    "        # Fit and transform the column using LabelEncoder\n",
    "        data[column] = le.fit_transform(data[column])\n",
    "\n",
    "# Now, the non-numeric columns have been converted to numerical labels\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O391HSvwo8dM"
   },
   "outputs": [],
   "source": [
    "data['SourceIP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDA-Zc3Xo9oD"
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhFUvRLia_uq"
   },
   "outputs": [],
   "source": [
    "# Separate the data into different classes\n",
    "benign_data = data[data['labels'] == 0].head(100)\n",
    "malicious_data = data[data['labels'] != 0].head(300)\n",
    "\n",
    "# Combine the data samples\n",
    "small_sample = pd.concat([benign_data, malicious_data], ignore_index=True)\n",
    "\n",
    "# Print the small sample\n",
    "# data = small_sample.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uYU-On4Uo_j-"
   },
   "outputs": [],
   "source": [
    "# Create the feature variables (X) by dropping the \"TimeStamp\" and \"labels\" columns from the DataFrame 'data'\n",
    "#X = data.drop([\"TimeStamp\", \"labels\"], axis=1)\n",
    "X = data.drop([\"TimeStamp\", \"labels\"], axis=1)\n",
    "# 'data.drop([\"TimeStamp\", \"labels\"], axis=1)' removes the \"TimeStamp\" and \"labels\" columns from 'data' and returns a new DataFrame 'X'\n",
    "# The 'axis=1' parameter specifies that we want to drop columns, not rows.\n",
    "\n",
    "# Create the target variable (y) by extracting the values from the \"labels\" column of the DataFrame 'data'\n",
    "#y = data['labels'].values\n",
    "y = data['labels'].values\n",
    "\n",
    "# 'data['labels']' accesses the \"labels\" column in 'data', and '.values' extracts the values as a NumPy array.\n",
    "# The resulting 'y' will be a one-dimensional NumPy array containing the target labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KOHlhfLnpBL_"
   },
   "outputs": [],
   "source": [
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lvq34CbSpCg5"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets using a test size of 50% (0.5) of the entire dataset\n",
    "# The random_state parameter ensures reproducibility by fixing the random seed used for the split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.5, random_state=1)\n",
    "\n",
    "# Further split the training set into training and validation sets using a test size of 25% (0.25) of the training set\n",
    "# The random_state parameter ensures consistency between different runs by using the same random seed as before.\n",
    "# The validation set size will be 25% of 50% (0.25 x 0.5 = 0.125) of the entire dataset.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_Y3wpaNpEGG"
   },
   "outputs": [],
   "source": [
    "\n",
    "def displayClasificationResults(z, y_test, y_pred, numClasses=4):\n",
    "    # Calculate and display the number of mislabeled points and accuracy\n",
    "    print(\"Number of mislabeled points out of a total %d points: %d\"\n",
    "          % (y_test.shape[0], (y_test != y_pred).sum()))\n",
    "    accuracy = round(100 - (((y_test != y_pred).sum() / y_test.shape[0]) * 100), 2)\n",
    "    print(f\"Accuracy is {accuracy}%\")\n",
    "\n",
    "    # Calculate and display precision, recall, and F-score (weighted average)\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average='weighted')\n",
    "    precision *= 100\n",
    "    recall *= 100\n",
    "    fscore *= 100\n",
    "    print(f\"Precision = {round(precision, 2)}%\")\n",
    "    print(f\"Recall = {round(recall, 2)}%\")\n",
    "    print(f\"F-score = {round(fscore, 2)}%\")\n",
    "\n",
    "    # Set the labels for x and y axes in the confusion matrix\n",
    "    if numClasses == 2:\n",
    "        x = ['benign', 'malicious']\n",
    "        y = ['benign', 'malicious']\n",
    "    else:\n",
    "        x = ['benign', 'iodine', 'dns2tcp', 'dnscat2']\n",
    "        y = ['benign', 'iodine', 'dns2tcp', 'dnscat2']\n",
    "\n",
    "    # Change each element of z to type string for annotations in the heatmap\n",
    "    z_text = [[str(y) for y in x] for x in z]\n",
    "\n",
    "    # Create an annotated heatmap using Plotly with the confusion matrix\n",
    "    fig = ff.create_annotated_heatmap(\n",
    "        z, x=x, y=y, annotation_text=z_text, colorscale='Viridis')\n",
    "\n",
    "    # Add title and custom axis titles to the heatmap\n",
    "    fig.update_layout(title_text='<i><b>Confusion matrix</b></i>')\n",
    "\n",
    "    # Add custom x-axis title\n",
    "    fig.add_annotation(dict(font=dict(color=\"black\", size=14),\n",
    "                            x=0.5,\n",
    "                            y=-0.15,\n",
    "                            showarrow=False,\n",
    "                            text=\"Predicted value\",\n",
    "                            xref=\"paper\",\n",
    "                            yref=\"paper\"))\n",
    "\n",
    "    # Add custom y-axis title with angle adjustment\n",
    "    fig.add_annotation(dict(font=dict(color=\"black\", size=14),\n",
    "                            x=-0.35,\n",
    "                            y=0.5,\n",
    "                            showarrow=False,\n",
    "                            text=\"Real value\",\n",
    "                            textangle=-90,\n",
    "                            xref=\"paper\",\n",
    "                            yref=\"paper\"))\n",
    "\n",
    "    # Adjust margins to make room for the y-axis title\n",
    "    fig.update_layout(margin=dict(t=50, l=200))\n",
    "\n",
    "    # Add colorbar to the heatmap\n",
    "    fig['data'][0]['showscale'] = True\n",
    "\n",
    "    # Show the heatmap\n",
    "    iplot(fig)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZRQvlGQJTeJ7"
   },
   "outputs": [],
   "source": [
    "# Create a RandomForestClassifier with 500 estimators and a fixed random state for reproducibility\n",
    "# rfc_4_classification = RandomForestClassifier(n_estimators=500, random_state=1)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc_4_classification = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    max_features='sqrt',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5C21p_LMWV_8"
   },
   "outputs": [],
   "source": [
    "# Train the RandomForestClassifier on the training data (X_train, y_train) and make predictions on the training data\n",
    "y_pred = rfc_4_classification.fit(X_train, y_train).predict(X_train)\n",
    "\n",
    "# Compute the confusion matrix using the actual training labels (y_train) and the predicted labels (y_pred)\n",
    "z = confusion_matrix(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yrKEWhCepQfl"
   },
   "outputs": [],
   "source": [
    "# Display the classification results using the 'displayClasificationResults' function\n",
    "# The function will show the number of mislabeled points, accuracy, precision, recall, and an annotated heatmap of the confusion matrix.\n",
    "displayClasificationResults(z, y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_Y0HNazpSGY"
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test data (X_test) using the trained RandomForestClassifier\n",
    "y_pred = rfc_4_classification.predict(X_test)\n",
    "\n",
    "# Compute the confusion matrix using the actual test labels (y_test) and the predicted labels (y_pred)\n",
    "z = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display the classification results using the 'displayClasificationResults' function\n",
    "# The function will show the number of mislabeled points, accuracy, precision, recall, and an annotated heatmap of the confusion matrix.\n",
    "displayClasificationResults(z, y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EhYEw5o8pTNu"
   },
   "outputs": [],
   "source": [
    "# Make predictions on the validation data (X_val) using the trained RandomForestClassifier\n",
    "y_pred = rfc_4_classification.predict(X_val)\n",
    "\n",
    "# Compute the confusion matrix using the actual validation labels (y_val) and the predicted labels (y_pred)\n",
    "z = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "# Display the classification results using the 'displayClasificationResults' function\n",
    "# The function will show the number of mislabeled points, accuracy, precision, recall, and an annotated heatmap of the confusion matrix.\n",
    "displayClasificationResults(z, y_val, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIBgVRRVpa_U"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate permutation feature importance\n",
    "perm_importance = permutation_importance(rfc_4_classification, X_test, y_test, n_repeats=30, random_state=1)\n",
    "\n",
    "# Obtain feature names\n",
    "feature_names = list(X.columns)\n",
    "\n",
    "# Sort features by importance scores\n",
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(sorted_idx)), perm_importance.importances_mean[sorted_idx], align='center')\n",
    "plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])\n",
    "plt.xlabel('Permutation Importance')\n",
    "plt.title('Feature Importance - Permutation Importance')\n",
    "plt.show()\n",
    "# Print feature names and their importance values in descending order\n",
    "for idx in reversed(sorted_idx):\n",
    "    print(f\"{feature_names[idx]}: {perm_importance.importances_mean[idx]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clv2TXVKyZQN"
   },
   "outputs": [],
   "source": [
    "# Print feature names and their importance values in descending order\n",
    "for idx in reversed(sorted_idx):\n",
    "    print(f\"{feature_names[idx]}: {perm_importance.importances_mean[idx]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJsXYYkK1Iah"
   },
   "outputs": [],
   "source": [
    "# Extract the top five feature names and their importance values\n",
    "top_feature_indices = sorted_idx[-5:]\n",
    "top_feature_names = [feature_names[idx] for idx in top_feature_indices]\n",
    "top_feature_importances = [perm_importance.importances_mean[idx] for idx in top_feature_indices]\n",
    "\n",
    "# Print the top five features and their importance values\n",
    "print(\"Top Five Features and Their Importance Values:\")\n",
    "for feature, importance in zip(top_feature_names, top_feature_importances):\n",
    "    print(f\"{feature}: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EKix7ZXt1LKo"
   },
   "outputs": [],
   "source": [
    "# Create a deep copy of data and name it newdata\n",
    "newdata = data.copy()\n",
    "\n",
    "# Create new combinations of the top five features using the mean\n",
    "new_feature_combinations = []\n",
    "for i in range(5):\n",
    "    for j in range(i + 1, 5):\n",
    "        new_combination = f\"{top_feature_names[i]}_{top_feature_names[j]}_mean\"\n",
    "        new_feature_combinations.append(new_combination)\n",
    "\n",
    "# Add the new feature combinations (mean) to the newdata dataframe\n",
    "for combination in new_feature_combinations:\n",
    "    feature_indices = [top_feature_names.index(name) for name in combination.split('_')[:-1]]\n",
    "    newdata[combination] = newdata[top_feature_names].iloc[:, feature_indices].mean(axis=1)\n",
    "\n",
    "\n",
    "# Print the first few rows of the dataframe to verify the additions\n",
    "print(\"Updated DataFrame with New Feature Combinations:\")\n",
    "newdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BBv73up5iuy"
   },
   "outputs": [],
   "source": [
    "# Prepare the new features and labels for machine learning\n",
    "X_new = newdata[new_feature_combinations].values\n",
    "y_new = newdata['labels'].values\n",
    "\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "X_train_all, X_temp_all, y_train_all, y_temp_all = train_test_split(X_new, y_new, test_size=0.5, random_state=1)\n",
    "X_val_all, X_test_all, y_val_all, y_test_all = train_test_split(X_temp_all, y_temp_all, test_size=0.25, random_state=1)\n",
    "\n",
    "# Create a new RandomForestClassifier for the updated dataset\n",
    "# rfc_new_all = RandomForestClassifier(n_estimators=500, random_state=1)\n",
    "rfc_new_all = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    max_features='sqrt',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "# Train the new classifier on the training data\n",
    "rfc_new_all.fit(X_train_all, y_train_all)\n",
    "\n",
    "# Make predictions on the training, validation, and testing data\n",
    "y_pred_train_all = rfc_new_all.predict(X_train_all)\n",
    "y_pred_val_all = rfc_new_all.predict(X_val_all)\n",
    "y_pred_test_all = rfc_new_all.predict(X_test_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBRMZ_TN1ZQV"
   },
   "outputs": [],
   "source": [
    "# Display classification results for the training data\n",
    "print(\"Classification Results for Training Data:\")\n",
    "displayClasificationResults(confusion_matrix(y_train_all, y_pred_train_all), y_train_all, y_pred_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZaTCASNs0iHr"
   },
   "outputs": [],
   "source": [
    "# Display classification results for the validation data\n",
    "print(\"Classification Results for Validation Data:\")\n",
    "displayClasificationResults(confusion_matrix(y_val_all, y_pred_val_all), y_val_all, y_pred_val_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YoOq4Th06iRx"
   },
   "outputs": [],
   "source": [
    "# Display classification results for the testing data\n",
    "print(\"Classification Results for Testing Data:\")\n",
    "displayClasificationResults(confusion_matrix(y_test_all, y_pred_test_all), y_test_all, y_pred_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4udLDOKD3Ks5"
   },
   "outputs": [],
   "source": [
    "# Create the feature variables (X) by dropping the \"TimeStamp\" and \"labels\" columns from the DataFrame 'data'\n",
    "X_all = newdata.drop([\"TimeStamp\", \"labels\"], axis=1)\n",
    "\n",
    "# Create the target variable (y) by extracting the values from the \"labels\" column of the DataFrame 'data'\n",
    "y_all = newdata['labels'].values\n",
    "X_imputed_all = imputer.fit_transform(X_all)\n",
    "X_scaled_all = scaler.fit_transform(X_imputed_all)\n",
    "\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "X_train_all, X_temp_all, y_train_all, y_temp_all = train_test_split(X_scaled_all, y_all, test_size=0.5, random_state=1)\n",
    "X_val_all, X_test_all, y_val_all, y_test_all = train_test_split(X_temp_all, y_temp_all, test_size=0.25, random_state=1)\n",
    "\n",
    "# Create a new RandomForestClassifier for the updated dataset\n",
    "# rfc_new_all = RandomForestClassifier(n_estimators=500, random_state=1)\n",
    "rfc_new_all = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    max_features='sqrt',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "# Train the new classifier on the training data\n",
    "rfc_new_all.fit(X_train_all, y_train_all)\n",
    "\n",
    "# Make predictions on the training, validation, and testing data\n",
    "y_pred_train_all = rfc_new_all.predict(X_train_all)\n",
    "y_pred_val_all = rfc_new_all.predict(X_val_all)\n",
    "y_pred_test_all = rfc_new_all.predict(X_test_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVCki0pZ8isk"
   },
   "outputs": [],
   "source": [
    "# Display classification results for the training data\n",
    "print(\"Classification Results for Training Data:\")\n",
    "displayClasificationResults(confusion_matrix(y_train_all, y_pred_train_all), y_train_all, y_pred_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ERXoiMb48lWZ"
   },
   "outputs": [],
   "source": [
    "# Display classification results for the validation data\n",
    "print(\"Classification Results for Validation Data:\")\n",
    "displayClasificationResults(confusion_matrix(y_val_all, y_pred_val_all), y_val_all, y_pred_val_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNJ56Xh78qoA"
   },
   "outputs": [],
   "source": [
    "# Display classification results for the testing data\n",
    "print(\"Classification Results for Testing Data:\")\n",
    "displayClasificationResults(confusion_matrix(y_test_all, y_pred_test_all), y_test_all, y_pred_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XijlrNi_-qtO"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calculate permutation feature importance\n",
    "perm_importance = permutation_importance(rfc_new_all, X_test_all, y_test_all, n_repeats=30, random_state=1)\n",
    "\n",
    "# Obtain feature names\n",
    "feature_names = list(X_all.columns)\n",
    "\n",
    "# Sort features by importance scores\n",
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(sorted_idx)), perm_importance.importances_mean[sorted_idx], align='center')\n",
    "plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])\n",
    "plt.xlabel('Permutation Importance')\n",
    "plt.title('Feature Importance - Permutation Importance')\n",
    "plt.show()\n",
    "# Print feature names and their importance values in descending order\n",
    "for idx in reversed(sorted_idx):\n",
    "    print(f\"{feature_names[idx]}: {perm_importance.importances_mean[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================================\n",
    "#\n",
    "#  Enhanced DNS Tunneling Detection: Feature Selection using Metaheuristics\n",
    "#\n",
    "#  Author:      [Mahmoud Sammour / UTeM]\n",
    "#  Date:        August 4, 2025\n",
    "#  Version:     2.1 (Modified for comparative visualization)\n",
    "#\n",
    "#  Project:\n",
    "#  This script applies and compares two advanced metaheuristic algorithms,\n",
    "#  RLGWO and its enhanced version, to perform feature selection for a\n",
    "#  DNS tunneling detection model. The goal is to identify a minimal, yet highly\n",
    "#  predictive, subset of features to improve model efficiency and performance.\n",
    "#\n",
    "#  Methodology:\n",
    "#  This project uses a public dataset for DNS-over-HTTPS (DoH) traffic analysis,\n",
    "#  combining benign Browse data with three types of malicious DNS tunneling\n",
    "#  traffic. This creates a multi-class classification problem with a significant\n",
    "#  class imbalance, justifying the use of a weighted F1-score for evaluation.\n",
    "#  - Classes: Benign, Iodine, DNS2TCP, Dnscat2.\n",
    "#\n",
    "#  The dataset, containing approximately 1.17 million samples, is divided into\n",
    "#  three sets for robust model development and evaluation:\n",
    "#  - Training Set:   50%   (~584k samples)\n",
    "#  - Validation Set: 37.5% (~438k samples)\n",
    "#  - Test Set:       12.5% (~146k samples)\n",
    "#\n",
    "#  A two-phase approach is used to balance optimization speed and solution quality:\n",
    "#  1.  Optimization Phase: A fast, lightweight Random Forest model is used as a\n",
    "#      fitness proxy to allow the optimizers to explore the vast search space\n",
    "#      of feature combinations quickly.\n",
    "#  2.  Validation Phase: The best feature set discovered by each algorithm is then\n",
    "#      validated using a full-sized, robust Random Forest model to determine its\n",
    "#      true performance.\n",
    "#\n",
    "# =======================================================================================\n",
    "\n",
    "# --- Core Libraries ---\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Machine Learning & Metrics ---\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# --- Deep Learning (PyTorch) ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# --- Visualization ---\n",
    "from tqdm.notebook import tqdm\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "\n",
    "# =======================================================================================\n",
    "# --- Section 1: Fitness Evaluation & Binary Adaptation Helpers ---\n",
    "# =======================================================================================\n",
    "\n",
    "def evaluate_features_fast(individual):\n",
    "    \"\"\"\n",
    "    Calculates the fitness of a feature set for the optimizers.\n",
    "\n",
    "    This function serves as a fast \"proxy\" for model performance. It uses a\n",
    "    lightweight Random Forest model to quickly evaluate a given solution. The\n",
    "    fitness score is a trade-off between the model's predictive power (F1-score)\n",
    "    and its complexity (number of features), promoting simpler, more generalizable\n",
    "    models.\n",
    "\n",
    "    Args:\n",
    "        individual (list): A binary list where a `1` at index `i` indicates that\n",
    "                           feature `i` is selected for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing a single float value representing the fitness.\n",
    "               The comma is required for compatibility with some optimization libraries.\n",
    "    \"\"\"\n",
    "    selected_indices = [i for i, selected in enumerate(individual) if selected]\n",
    "    if not selected_indices:\n",
    "        return 0.0,  # Return the worst possible fitness if no features are selected.\n",
    "\n",
    "    # Slice the global dataset variables to use only the selected features.\n",
    "    X_train_subset = X_train_all[:, selected_indices]\n",
    "    X_test_subset = X_test_all[:, selected_indices]\n",
    "\n",
    "    # Define a lightweight classifier. Key parameters (n_estimators, max_depth) are\n",
    "    # kept low to ensure this evaluation step is fast.\n",
    "    classifier = RandomForestClassifier(n_estimators=20, max_depth=8, random_state=42, n_jobs=-1)\n",
    "    classifier.fit(X_train_subset, y_train_all)\n",
    "\n",
    "    y_pred = classifier.predict(X_test_subset)\n",
    "    score = f1_score(y_test_all, y_pred, average='weighted')\n",
    "\n",
    "    # Apply a penalty for model complexity. This is the correct penalty for this problem.\n",
    "    penalty = 0.05 * (sum(individual) / len(individual))\n",
    "\n",
    "    return score - penalty,\n",
    "\n",
    "\n",
    "def _binarize_wolf(continuous_position):\n",
    "    \"\"\"\n",
    "    Converts a continuous position vector from GWO into a binary vector.\n",
    "\n",
    "    This is the core adaptation that allows a continuous optimizer like GWO to solve\n",
    "    a binary problem. It uses a sigmoid transfer function to map the agent's\n",
    "    continuous position to a vector of probabilities. A stochastic threshold then\n",
    "    converts these probabilities into a definitive binary choice (0 or 1) for each feature.\n",
    "\n",
    "    Args:\n",
    "        continuous_position (np.ndarray): The GWO agent's position in continuous space.\n",
    "\n",
    "    Returns:\n",
    "        list: The corresponding binary position vector for feature selection.\n",
    "    \"\"\"\n",
    "    # Sigmoid function squashes any real number into a (0, 1) probability.\n",
    "    probabilities = 1 / (1 + np.exp(-np.array(continuous_position)))\n",
    "    \n",
    "    # For each feature, if a random number is less than its probability, select it (1).\n",
    "    binary_position = (np.random.random(len(probabilities)) < probabilities).astype(int)\n",
    "\n",
    "    # A solution with zero features is invalid. If this occurs, randomly flip one\n",
    "    # gene to 1 to ensure validity.\n",
    "    if np.sum(binary_position) == 0:\n",
    "        binary_position[np.random.randint(0, len(binary_position))] = 1\n",
    "\n",
    "    return binary_position.tolist()\n",
    "\n",
    "\n",
    "def binary_opposition_based_learning(population):\n",
    "    \"\"\"\n",
    "    Generates the \"opposite\" of a binary population by flipping all bits.\n",
    "\n",
    "    This strategy is used as a powerful exploration mechanism, allowing the optimizer\n",
    "    to escape local optima by making a large jump to a completely different region\n",
    "    of the search space.\n",
    "\n",
    "    Args:\n",
    "        population (list of lists): The current population of binary individuals.\n",
    "\n",
    "    Returns:\n",
    "        list of lists: A new population where each individual is the bitwise\n",
    "                       complement of an individual in the original population.\n",
    "    \"\"\"\n",
    "    return [[1 - bit for bit in individual] for individual in population]\n",
    "\n",
    "\n",
    "# =======================================================================================\n",
    "# --- Section 2: Deep Reinforcement Learning Architecture ---\n",
    "# =======================================================================================\n",
    "\n",
    "class DuelingDQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a Dueling Deep Q-Network (DQN) architecture.\n",
    "\n",
    "    A Dueling DQN has two streams to separately estimate state values (V(s)) and\n",
    "    action advantages (A(s,a)). This separation allows the network to learn which\n",
    "    states are (or are not) valuable without having to learn the effect of each\n",
    "    action for each state. This is particularly useful in environments where actions\n",
    "    do not affect the environment in any relevant way.\n",
    "\n",
    "    Reference:\n",
    "        Wang, Z., Schaul, T., Hessel, M., et al. (2016). \"Dueling Network\n",
    "        Architectures for Deep Reinforcement Learning.\" PMLR.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.feature_layer = nn.Sequential(nn.Linear(state_size, 64), nn.ReLU())\n",
    "        \n",
    "        # The Value Stream: predicts V(s) - how good is it to be in this state?\n",
    "        self.value_stream = nn.Sequential(nn.Linear(64, 64), nn.ReLU(), nn.Linear(64, 1))\n",
    "        \n",
    "        # The Advantage Stream: predicts A(s,a) - how much better is taking this\n",
    "        # action compared to the other possible actions?\n",
    "        self.advantage_stream = nn.Sequential(nn.Linear(64, 64), nn.ReLU(), nn.Linear(64, action_size))\n",
    "\n",
    "    def forward(self, state):\n",
    "        features = self.feature_layer(state)\n",
    "        value = self.value_stream(features)\n",
    "        advantages = self.advantage_stream(features)\n",
    "        \n",
    "        # Combine value and advantages to get the final Q-values. The mean of the\n",
    "        # advantages is subtracted to ensure identifiability.\n",
    "        return value + (advantages - advantages.mean(dim=1, keepdim=True))\n",
    "\n",
    "\n",
    "# =======================================================================================\n",
    "# --- Section 3: Optimizer Algorithm Implementations ---\n",
    "# =======================================================================================\n",
    "\n",
    "class BinaryRLGWO:\n",
    "    \"\"\"\n",
    "    A Reinforcement Learning-Guided Grey Wolf Optimizer (RLGWO) adapted for\n",
    "    binary feature selection problems. It uses a simple Q-table to learn a\n",
    "    policy for balancing exploration and exploitation.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_dimensions, population_size, generations):\n",
    "        self.n_dimensions, self.population_size, self.generations = n_dimensions, population_size, generations\n",
    "        self.actions = [\"exploration\", \"exploitation\"]\n",
    "        self.q_table = defaultdict(lambda: 0)\n",
    "        self.epsilon, self.alpha, self.gamma = 0.2, 0.1, 0.9\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.random() < self.epsilon: return random.choice(self.actions)\n",
    "        return self.actions[np.argmax([self.q_table[(state, action)] for action in self.actions])]\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        max_next_q = max(self.q_table.get((next_state, a), 0) for a in self.actions)\n",
    "        self.q_table[(state, action)] += self.alpha * (reward + self.gamma * max_next_q - self.q_table[(state, action)])\n",
    "\n",
    "    def run(self, evaluate_func):\n",
    "        population = [[random.randint(0, 1) for _ in range(self.n_dimensions)] for _ in range(self.population_size)]\n",
    "        fitnesses = [evaluate_func(ind)[0] for ind in population]\n",
    "        state = f\"{max(fitnesses):.3f}_{np.std(fitnesses):.3f}\"\n",
    "        best_individual_so_far = population[np.argmax(fitnesses)]\n",
    "        best_fitness_so_far = max(fitnesses)\n",
    "        history = [best_fitness_so_far]\n",
    "        progress_bar = tqdm(range(1, self.generations + 1), desc=\"Standard RLGWO\")\n",
    "        for gen in progress_bar:\n",
    "            action = self.choose_action(state)\n",
    "            a_factor = 1.5 if action == \"exploration\" else 0.5\n",
    "            pop_with_fitness = sorted(zip(population, fitnesses), key=lambda x: x[1], reverse=True)\n",
    "            alpha, beta, delta = pop_with_fitness[0][0], pop_with_fitness[1][0], pop_with_fitness[2][0]\n",
    "            new_population = []\n",
    "            for i in range(self.population_size):\n",
    "                A1,A2,A3 = a_factor*(2*np.random.rand(self.n_dimensions)-1), a_factor*(2*np.random.rand(self.n_dimensions)-1), a_factor*(2*np.random.rand(self.n_dimensions)-1)\n",
    "                C1,C2,C3 = 2*np.random.rand(self.n_dimensions), 2*np.random.rand(self.n_dimensions), 2*np.random.rand(self.n_dimensions)\n",
    "                D_alpha, D_beta, D_delta = np.abs(C1*np.array(alpha) - population[i]), np.abs(C2*np.array(beta) - population[i]), np.abs(C3*np.array(delta) - population[i])\n",
    "                X1,X2,X3 = np.array(alpha)-A1*D_alpha, np.array(beta)-A2*D_beta, np.array(delta)-A3*D_delta\n",
    "                new_population.append(_binarize_wolf((X1 + X2 + X3) / 3.0))\n",
    "            population = new_population; fitnesses = [evaluate_func(ind)[0] for ind in population]\n",
    "            if max(fitnesses) > best_fitness_so_far:\n",
    "                best_fitness_so_far = max(fitnesses); best_individual_so_far = population[np.argmax(fitnesses)]\n",
    "            history.append(best_fitness_so_far)\n",
    "            reward = max(fitnesses) + 0.1 * np.std(fitnesses)\n",
    "            next_state = f\"{max(fitnesses):.3f}_{np.std(fitnesses):.3f}\"\n",
    "            self.update_q_table(state, action, reward, next_state); state = next_state\n",
    "            progress_bar.set_postfix(best_fitness=f\"{best_fitness_so_far:.4f}\", features=f\"{sum(best_individual_so_far)}\")\n",
    "        return best_individual_so_far, best_fitness_so_far, history\n",
    "\n",
    "class EnhancedBinaryRLGWO:\n",
    "    \"\"\"\n",
    "    An enhanced RLGWO adapted for binary problems, featuring a Dueling DQN,\n",
    "    Prioritized Experience Replay (PER), and Opposition-Based Learning (OBL).\n",
    "    This represents the full, non-simplified version of the algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_dimensions, population_size, generations):\n",
    "        self.n_dimensions = n_dimensions\n",
    "        self.population_size = population_size\n",
    "        self.generations = generations\n",
    "        # --- Algorithm Actions ---\n",
    "        # The agent can choose to modulate the GWO 'a' parameter for exploration/exploitation\n",
    "        # or trigger a strategic jump with Opposition-Based Learning.\n",
    "        self.actions = [(\"set_a\", 0.5), (\"set_a\", 1.0), (\"set_a\", 1.5), (\"activate_obl\", None)]\n",
    "        # --- RL Hyperparameters ---\n",
    "        self.epsilon, self.epsilon_end, self.epsilon_decay = 1.0, 0.01, 0.995\n",
    "        self.gamma, self.tau, self.batch_size = 0.99, 0.005, 64\n",
    "        # --- PER Hyperparameters ---\n",
    "        self.replay_buffer_size, self.beta, self.priority_epsilon = 2000, 0.4, 1e-6\n",
    "        self.replay_buffer = []\n",
    "        # --- PyTorch Setup ---\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.q_network = DuelingDQN(3, len(self.actions)).to(self.device)\n",
    "        self.target_q_network = DuelingDQN(3, len(self.actions)).to(self.device)\n",
    "        self.target_q_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.001)\n",
    "    \n",
    "    def add_to_buffer(self, experience):\n",
    "        max_priority = max([p for p, _ in self.replay_buffer]) if self.replay_buffer else 1.0\n",
    "        self.replay_buffer.append((max_priority, experience))\n",
    "        if len(self.replay_buffer) > self.replay_buffer_size: self.replay_buffer.pop(0)\n",
    "\n",
    "    ### NON-SIMPLIFIED: Full Prioritized Experience Replay (PER) ###\n",
    "    def replay(self):\n",
    "        \"\"\"\n",
    "        Trains the DQN using Prioritized Experience Replay (PER), which samples\n",
    "        more important transitions more frequently.\n",
    "\n",
    "        Reference:\n",
    "            Schaul, T., Quan, J., Antonoglou, I., & Silver, D. (2015).\n",
    "            \"Prioritized Experience Replay.\" arXiv.\n",
    "        \"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size: return\n",
    "        \n",
    "        priorities = np.array([p for p, _ in self.replay_buffer]); probs = priorities ** self.beta; probs /= probs.sum()\n",
    "        indices = np.random.choice(len(self.replay_buffer), self.batch_size, p=probs)\n",
    "        batch = [self.replay_buffer[i][1] for i in indices]\n",
    "        \n",
    "        weights = (len(self.replay_buffer) * probs[indices]) ** (-self.beta); weights /= weights.max()\n",
    "        weights = torch.tensor(weights, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        states, actions, rewards, next_states = zip(*batch)\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32).to(self.device)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
    "        action_indices = torch.tensor(actions, dtype=torch.int64).to(self.device)\n",
    "        \n",
    "        predicted_q = self.q_network(states).gather(1, action_indices.unsqueeze(1)).squeeze(1)\n",
    "        with torch.no_grad(): next_q_values = self.target_q_network(next_states).max(1)[0]\n",
    "        target_q = rewards + self.gamma * next_q_values\n",
    "        td_errors = torch.abs(target_q - predicted_q).detach()\n",
    "        loss = (weights * nn.MSELoss(reduction='none')(predicted_q, target_q)).mean()\n",
    "        \n",
    "        self.optimizer.zero_grad(); loss.backward(); self.optimizer.step()\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            self.replay_buffer[idx] = (td_errors[i].item() + self.priority_epsilon, self.replay_buffer[idx][1])\n",
    "            \n",
    "        for target_param, local_param in zip(self.target_q_network.parameters(), self.q_network.parameters()):\n",
    "            target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "    def choose_action_index(self, state):\n",
    "        if random.random() < self.epsilon: return random.randrange(len(self.actions))\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).to(self.device).unsqueeze(0)\n",
    "        with torch.no_grad(): q_values = self.q_network(state_tensor)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def run(self, evaluate_func):\n",
    "        \"\"\"Executes the optimization process.\"\"\"\n",
    "        population = [[random.randint(0, 1) for _ in range(self.n_dimensions)] for _ in range(self.population_size)]\n",
    "        fitnesses = [evaluate_func(ind)[0] for ind in population]\n",
    "        state = [max(fitnesses), np.mean(fitnesses), np.std(fitnesses)]\n",
    "        best_individual_so_far = population[np.argmax(fitnesses)]; best_fitness_so_far = max(fitnesses)\n",
    "        history = [best_fitness_so_far]\n",
    "\n",
    "        progress_bar = tqdm(range(1, self.generations + 1), desc=\"Enhanced RLGWO\")\n",
    "        for gen in progress_bar:\n",
    "            self.beta = min(1.0, self.beta + 0.001)\n",
    "            action_index = self.choose_action_index(state)\n",
    "            action_type, action_value = self.actions[action_index]\n",
    "            \n",
    "            if action_type == \"activate_obl\":\n",
    "                opposite_pop = binary_opposition_based_learning(population); combined_pop = population + opposite_pop\n",
    "                combined_fitnesses = [evaluate_func(ind)[0] for ind in combined_pop]\n",
    "                sorted_combined = sorted(zip(combined_pop, combined_fitnesses), key=lambda x: x[1], reverse=True)\n",
    "                population = [ind for ind, fit in sorted_combined[:self.population_size]]\n",
    "                fitnesses = [fit for ind, fit in sorted_combined[:self.population_size]]\n",
    "            else: # \"set_a\"\n",
    "                a_factor = action_value\n",
    "                pop_with_fitness = sorted(zip(population, fitnesses), key=lambda x: x[1], reverse=True)\n",
    "                alpha, beta, delta = pop_with_fitness[0][0], pop_with_fitness[1][0], pop_with_fitness[2][0]\n",
    "                new_population = []\n",
    "                for i in range(self.population_size):\n",
    "                    A1,A2,A3 = a_factor*(2*np.random.rand(self.n_dimensions)-1),a_factor*(2*np.random.rand(self.n_dimensions)-1),a_factor*(2*np.random.rand(self.n_dimensions)-1)\n",
    "                    C1,C2,C3 = 2*np.random.rand(self.n_dimensions), 2*np.random.rand(self.n_dimensions), 2*np.random.rand(self.n_dimensions)\n",
    "                    D_alpha=np.abs(C1*np.array(alpha)-population[i]); D_beta=np.abs(C2*np.array(beta)-population[i]); D_delta=np.abs(C3*np.array(delta)-population[i])\n",
    "                    X1=np.array(alpha)-A1*D_alpha; X2=np.array(beta)-A2*D_beta; X3=np.array(delta)-A3*D_delta\n",
    "                    new_population.append(_binarize_wolf((X1 + X2 + X3) / 3.0))\n",
    "                population = new_population; fitnesses = [evaluate_func(ind)[0] for ind in population]\n",
    "            \n",
    "            if max(fitnesses) > best_fitness_so_far:\n",
    "                best_fitness_so_far = max(fitnesses); best_individual_so_far = population[np.argmax(fitnesses)]\n",
    "            history.append(best_fitness_so_far)\n",
    "            \n",
    "            next_state = [max(fitnesses), np.mean(fitnesses), np.std(fitnesses)]\n",
    "            reward = next_state[0] + 0.1 * next_state[2]\n",
    "            self.add_to_buffer((state, action_index, reward, next_state)); self.replay(); state = next_state\n",
    "            self.epsilon = max(self.epsilon_end, self.epsilon_decay * self.epsilon)\n",
    "            progress_bar.set_postfix(best_fitness=f\"{best_fitness_so_far:.4f}\", features=f\"{sum(best_individual_so_far)}\")\n",
    "        \n",
    "        return best_individual_so_far, best_fitness_so_far, history\n",
    "\n",
    "# =======================================================================================\n",
    "# --- Section 4: Main Experiment Execution & Visualization ---\n",
    "# =======================================================================================\n",
    "\n",
    "### --- Phase 1: Run Fast Optimization --- ###\n",
    "print(\"--- Phase 1: Running Fast Optimization ---\")\n",
    "pop_size = 40\n",
    "generations = 15\n",
    "n_features = X_train_all.shape[1]\n",
    "\n",
    "# Assume feature_names is a list of strings corresponding to the columns of X_train_all\n",
    "# e.g., feature_names = ['feature_1', 'feature_2', ...]\n",
    "\n",
    "gwo = BinaryRLGWO(n_features, pop_size, generations)\n",
    "best_sol_gwo, _, history_gwo = gwo.run(evaluate_features_fast)\n",
    "\n",
    "enhanced_gwo = EnhancedBinaryRLGWO(n_features, pop_size, generations)\n",
    "best_sol_enhanced, _, history_enhanced = enhanced_gwo.run(evaluate_features_fast)\n",
    "print(\"\\n Optimization Phase Complete.\")\n",
    "\n",
    "\n",
    "### --- NEW HELPER: Full Validation and Plotting Function --- ###\n",
    "def validate_and_plot_results(solution_binary, algorithm_name, feature_names, X_train, y_train, X_test, y_test, class_names):\n",
    "    \"\"\"\n",
    "    Validates a feature set, prints metrics, and plots a confusion matrix.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\" VALIDATING: {algorithm_name} \")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    solution_indices = [i for i, selected in enumerate(solution_binary) if selected]\n",
    "    selected_feature_names = [feature_names[i] for i in solution_indices]\n",
    "\n",
    "    if not solution_indices:\n",
    "        print(\"No features were selected. Cannot validate model.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(selected_feature_names)} features. Training full-sized model...\")\n",
    "    X_train_final = X_train[:, solution_indices]\n",
    "    X_test_final = X_test[:, solution_indices]\n",
    "\n",
    "    # Define and train the full-sized, \"heavyweight\" classifier.\n",
    "    final_classifier = RandomForestClassifier(n_estimators=500, random_state=42, n_jobs=-1)\n",
    "    final_classifier.fit(X_train_final, y_train)\n",
    "    y_pred_final = final_classifier.predict(X_test_final)\n",
    "    print(f\" {algorithm_name} Model Trained and Evaluated.\")\n",
    "\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(classification_report(y_test, y_pred_final, target_names=class_names))\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\\n\")\n",
    "    cm = confusion_matrix(y_test, y_pred_final)\n",
    "    cm_text = [[str(y) for y in x] for x in cm]\n",
    "\n",
    "    fig_cm = ff.create_annotated_heatmap(\n",
    "        z=cm, x=class_names, y=class_names, annotation_text=cm_text, colorscale='Purples'\n",
    "    )\n",
    "    fig_cm.update_layout(\n",
    "        title_text=f'<b>{algorithm_name} Confusion Matrix</b>',\n",
    "        xaxis_title='Predicted Label',\n",
    "        yaxis_title='True Label'\n",
    "    )\n",
    "    fig_cm.show()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Features Selected by {algorithm_name} ({len(selected_feature_names)} total):\")\n",
    "    for name in selected_feature_names: print(f\"- {name}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "\n",
    "### --- Phase 2: Perform Full Validation on Best Solutions --- ###\n",
    "print(\"\\n\\n--- Phase 2: Performing Full Validation on Results from Each Algorithm ---\")\n",
    "class_names = ['Benign', 'Iodine', 'DNS2TCP', 'Dnscat2']\n",
    "\n",
    "# Validate the solution from the Standard RLGWO\n",
    "validate_and_plot_results(\n",
    "    solution_binary=best_sol_gwo,\n",
    "    algorithm_name=\"Standard RLGWO\",\n",
    "    feature_names=feature_names,\n",
    "    X_train=X_train_all,\n",
    "    y_train=y_train_all,\n",
    "    X_test=X_test_all,\n",
    "    y_test=y_test_all,\n",
    "    class_names=class_names\n",
    ")\n",
    "\n",
    "# Validate the solution from the Enhanced RLGWO\n",
    "validate_and_plot_results(\n",
    "    solution_binary=best_sol_enhanced,\n",
    "    algorithm_name=\"Enhanced RLGWO\",\n",
    "    feature_names=feature_names,\n",
    "    X_train=X_train_all,\n",
    "    y_train=y_train_all,\n",
    "    X_test=X_test_all,\n",
    "    y_test=y_test_all,\n",
    "    class_names=class_names\n",
    ")\n",
    "\n",
    "\n",
    "### --- Final Convergence Plot --- ###\n",
    "def plot_convergence(history_gwo, history_enhanced):\n",
    "    \"\"\"Plots the fitness history of both optimizers using Plotly.\"\"\"\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(y=history_gwo, mode='lines+markers', name='Standard RLGWO', line=dict(color='royalblue')))\n",
    "    fig.add_trace(go.Scatter(y=history_enhanced, mode='lines+markers', name='Enhanced RLGWO (with PER)', line=dict(color='firebrick')))\n",
    "    fig.update_layout(\n",
    "        title='<b>Optimizer Convergence During Fast Training</b>',\n",
    "        xaxis_title='Generation',\n",
    "        yaxis_title='Best Fitness (F1-Score - Penalty)',\n",
    "        legend_title='Algorithm',\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "print(\"\\n\\n--- Plotting Optimizer Convergence ---\")\n",
    "plot_convergence(history_gwo, history_enhanced)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1aKGMe24q-3zesXzsDR24ekqUnAwftg0u",
     "timestamp": 1692082397536
    },
    {
     "file_id": "1KGM_pWvnkfwxSetE0KOdhbnIrN1zkkky",
     "timestamp": 1691224448663
    }
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
